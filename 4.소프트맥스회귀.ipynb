{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4.소프트맥스회귀.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNN8Z1F5Bl6U+icQdd+nOys",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "efeff0032f414097914fbf733f36f4a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_8592d4b9e9d743edb02108a9b64f2127",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_8e0f7804b0484ea3a880963018811b5f",
              "IPY_MODEL_107e434374284679a7f716aee707c923"
            ]
          }
        },
        "8592d4b9e9d743edb02108a9b64f2127": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "8e0f7804b0484ea3a880963018811b5f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ac471eb8021d46c4878db59005d7a820",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "info",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_0f0f295d198541998270da5f06fe863e"
          }
        },
        "107e434374284679a7f716aee707c923": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_442274cfa52541d79571860f6ffe1173",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 9920512/? [00:20&lt;00:00, 1398602.03it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6df0311cd47d46c2a1ef75285ff448d5"
          }
        },
        "ac471eb8021d46c4878db59005d7a820": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "0f0f295d198541998270da5f06fe863e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "442274cfa52541d79571860f6ffe1173": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6df0311cd47d46c2a1ef75285ff448d5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9970a07a2b7d4303a22010d9a7573ea9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_0dad6fe3bbc8472d947f733638d1b44b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e771f7fa59bf485db7bb5b1dbd99a903",
              "IPY_MODEL_2114cadc761d46e5a1c26a3a86197fbb"
            ]
          }
        },
        "0dad6fe3bbc8472d947f733638d1b44b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e771f7fa59bf485db7bb5b1dbd99a903": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_ed85ab0e4f9e46b68d7b60f8618c4f9a",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_afe956f603754c04a8319479bccd9d3d"
          }
        },
        "2114cadc761d46e5a1c26a3a86197fbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_4939cb5328f545289d413535e897e779",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 32768/? [00:03&lt;00:00, 10152.98it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_671e0c95b1f2467285770a27fe279bf8"
          }
        },
        "ed85ab0e4f9e46b68d7b60f8618c4f9a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "afe956f603754c04a8319479bccd9d3d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "4939cb5328f545289d413535e897e779": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "671e0c95b1f2467285770a27fe279bf8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "77376caabc3d41b9a2684cde6d6766fd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_d177bdfa11ef47ecb1b08f9ad6ac1b19",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_e1bbec8dc2af40ba909e080664866887",
              "IPY_MODEL_b73236930af24b84a234ddb835273a97"
            ]
          }
        },
        "d177bdfa11ef47ecb1b08f9ad6ac1b19": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e1bbec8dc2af40ba909e080664866887": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_2eb3dfdfce94432b916e35b2cb2664fb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_69d5425e1ea648368e3dd6428b9b5311"
          }
        },
        "b73236930af24b84a234ddb835273a97": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_e19aa26f87db411f8c2004ddbba897f2",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 1654784/? [00:02&lt;00:00, 686167.08it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_22a19dbf8d4b4cf78172985263cc9ab7"
          }
        },
        "2eb3dfdfce94432b916e35b2cb2664fb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "69d5425e1ea648368e3dd6428b9b5311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "e19aa26f87db411f8c2004ddbba897f2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "22a19dbf8d4b4cf78172985263cc9ab7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "00a94c98892946b4979b4f17eb7529a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_94726422a2cb4ca290b0a62629f26b83",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_9232147ae52849f0afbb811dcde50140",
              "IPY_MODEL_44ea248dfc67413ea30697ccef78dd57"
            ]
          }
        },
        "94726422a2cb4ca290b0a62629f26b83": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9232147ae52849f0afbb811dcde50140": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_02868d2b879b4204af0d7b7087ab05a0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 1,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 1,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_5c222876aafa4949a06fb1df51f01506"
          }
        },
        "44ea248dfc67413ea30697ccef78dd57": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_d93a27f9a70f487c98bad5ff87e9f63b",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 8192/? [00:00&lt;00:00, 14126.00it/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_85e6dc73bf6c422da011d4ecb230249c"
          }
        },
        "02868d2b879b4204af0d7b7087ab05a0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "5c222876aafa4949a06fb1df51f01506": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "d93a27f9a70f487c98bad5ff87e9f63b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "85e6dc73bf6c422da011d4ecb230249c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jung0Jin/Pytorch_study/blob/master/4.%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4%ED%9A%8C%EA%B7%80.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiqLac1VzHi6",
        "colab_type": "text"
      },
      "source": [
        "출처 : https://github.com/Namsik-Yoon/pytorch_basic/blob/master/4.%20%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4%20%ED%9A%8C%EA%B7%80(Softmax_Regression).ipynb\n",
        "\n",
        "에서 가져왔다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vqZ_SeA4zOEu",
        "colab_type": "text"
      },
      "source": [
        "#4. 소프트맥스 회귀(Softmax Regression)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ZbsPYWh1YKY",
        "colab_type": "text"
      },
      "source": [
        "##4.1 원-핫 인코딩(One-Hot Encoding)\n",
        "\n",
        "범주형 데이터를 처리할 때 레이블을 표현하는 방법인 원-핫 인코딩에 대해 배우자."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSmt9oTb1rKP",
        "colab_type": "text"
      },
      "source": [
        "###4.1.1 원-핫 인코딩이란?\n",
        "\n",
        "원-핫 인코딩은 선택지의 개수만큼의 차원을 가지면서, 각 선택지의 인덱스에 해당하는 원소에 1, 나머지 원소는 0의 값을 가지도록 하는 표현 방법이다.\n",
        "\n",
        "예를 들어 강아지, 고양이, 냉장고가 있다하자.\n",
        "\n",
        "강아지 = [1, 0, 0]\n",
        "\n",
        "고양이 = [0, 1, 0]\n",
        "\n",
        "냉장고 = [0, 0, 1]\n",
        "\n",
        "로 표현하는게 원-핫 인코딩이다. 이 벡터들을 원-핫 벡터라고 한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qtzT3hhw2MlC",
        "colab_type": "text"
      },
      "source": [
        "###4.1.2 원-핫 벡터의 무작위성\n",
        "\n",
        "범주형 데이터가 각 데이터 간의 관계가 균등하다는 점에서 원-핫 벡터는 적절한 표현 방법이다.\n",
        "\n",
        "다중 클래스 분류 문제도 각 클래스 간의 관계가 균등하다는 점에서 적절한 표현 방법이다.\n",
        "\n",
        "예를 들어 강아지, 고양이, 냉장고를 정수 인코딩 (0, 1, 2) 으로  레이블 한다면, 강아지와 고양이 사이의 관계(1-0 = 1)가 강아지와 냉장고 사이의 관계(2-0 = 2)보다 가깝다고 생각될 수 있다.\n",
        "\n",
        "클래스 간의 관계가 균등하기 때문에 특정 클래스가 가깝다고 여겨지는 정수 인코딩보다 원-핫 인코딩이 클래스의 성질을 잘 표현했다고 할 수 있다.\n",
        "\n",
        "이걸 아래의 식인 MSE를 사용하여 확인해보자.\n",
        "\n",
        "$Loss\\ function = \\frac{1}{n} \\sum_i^{n} \\left(y_{i} - \\hat{y_{i}}\\right)^2$\n",
        "\n",
        "강아지, 고양이, 냉장고라는 3개의 클래스를 각각 0, 1, 2 로 정수 인코딩한다.\n",
        "\n",
        "실제값이 강아지, 예측값이 고양이면 MSE는 다음과 같다.\n",
        "\n",
        "$(1-0)^{2} = 1$\n",
        "\n",
        "실제값이 강아지, 예측값이 냉장고면 MSE는 다음과 같다.\n",
        "\n",
        "$(2-0)^{2} = 4$\n",
        "\n",
        "오차의 값이 다르다. 즉, 이는 기계에게 강아지가 냉장고보다 고양이에 가깝다는 정보를 주는 것과 다름 없다.\n",
        "\n",
        "더 많은 클래스에 대해 정수 인코딩을 수행하면? 난리난다.\n",
        "\n",
        "정리하겠다. 정수 인코딩과 달리 원-핫 인코딩은 분류 문제에서 모든 클래스 간의 관계를 균등하게 분배한다.\n",
        "\n",
        "원-핫 인코딩을 했을 때의 MSE를 보겠다.\n",
        "\n",
        "$((1,0,0)-(0,1,0))^{2} = (1-0)^{2} + (0-1)^{2} + (0-0)^{2} = 2$\n",
        "\n",
        "$((1,0,0)-(0,0,1))^{2} = (1-0)^{2} + (0-0)^{2} + (0-1)^{2} = 2$\n",
        "\n",
        "원-핫 인코딩은 클래스의 관계를 균등하기 만들기 때문에 원-핫벡터는 무작위성을 가진다. 각 클래스를 인코딩 할 때 표현 방법이 무작위하다는 것이다. 예를 들어 강아지가 (1,0,0) 일지 (0,1,0) 일지 (0,0,1) 일지 모른다. 이러한 원-핫 벡터의 무작위성은 때로는 단어의 유사성을 구할 수 없다는 단점으로 언급되기도 하는데 기억하지 말자ㅎ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5cOXMRLf5o-x",
        "colab_type": "text"
      },
      "source": [
        "## 4.2 소프트맥스 회귀 이해하기\n",
        "\n",
        "로지스틱 회귀 : 2개 중 1개 고르는 이진 분류\n",
        "\n",
        "소프트맥스 회귀 : 3개 이상 중 1개 고르는 다증 클래스 분류"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FO2iqi7v8XjZ",
        "colab_type": "text"
      },
      "source": [
        "###4.2.1 다중 클래스 분류\n",
        "\n",
        "아래는 꽃받침 길이, 꽃받침 넓이, 꽃잎 길이, 꽃잎 넓이라는 4개의 특성(feature)로부터 setosa, versicolor, virginica 라는 3개의 붓꽃 품종을 예측하는 문제이다.\n",
        "\n",
        "|SepalLengthCm$(x_1)$|SepalWidthCm$(x_2)$|PetalLengthCm$(x_3)$|PetalWidthCm$(x_4)$|Species$(y)$|\n",
        "|---|---|---|---|---|\n",
        "|5.1|3.5|1.4|0.2|setosa|\n",
        "|4.9|3.0|1.4|0.2|setosa|\n",
        "|5.8|2.6|4.0|1.2|versicolor|\n",
        "|6.7|3.0|5.2|2.3|virginica|\n",
        "|5.6|2.8|4.9|2.0|virginica|\n",
        "\n",
        "이번 챕터의 설명에서 입력은 $X$, 가중치는 $W$, 편향은 $B$, 출력은 $\\hat{Y}$로 각 변수는 벡터 또는 행렬로 가정한다.\n",
        "\n",
        "*   $\\hat{Y}$은 예측값이라는 의미를 가지고 있으므로 가설식에서 $H(X)$ 대신 사용되기도 한다.\n",
        "\n",
        "1) 로지스틱 회귀\n",
        "\n",
        "로지스틱 회귀에서 시그모이드 함수는 예측값을 0과 1사이의 값으로 만든다.\n",
        "\n",
        "예를 들어 스팸 메일 분류기를 로지스틱 회귀로 구현하면, 출력이 0.75라면 스팸일 확률이 75%라는 의미가 된다. 동시에, 스팸이 아닐 확률이 25%가 된다.\n",
        "\n",
        "![대체 텍스트](https://camo.githubusercontent.com/acee24af0ee38fcf4de25f774073d702ad289194/68747470733a2f2f77696b69646f63732e6e65742f696d616765732f706167652f35393432372f2545422541312539432545432541372538302545432538412541342545442538422542312545442539412538432545412542372538302e504e47)\n",
        "\n",
        "가설 : $H(X) = sigmoid(WX + B)$\n",
        "\n",
        "2) 소프트맥스 회귀\n",
        "\n",
        "소프트맥스 회귀에서 소프트맥스 함수는 예측값을 0과 1사이의 값으로 만든다.\n",
        "\n",
        "선택지의 개수만큼의 차원을 가지는 벡터를 만들고, 모든 원소의 합이 1이 되도록 원소들의 값을 변환시키는 소프트맥스 함수를 지나게 만들어야 한다.\n",
        "\n",
        "![대체 텍스트](https://camo.githubusercontent.com/2be4df15c4c65877caf65dced6533ed34009e39e/68747470733a2f2f77696b69646f63732e6e65742f696d616765732f706167652f35393432372f2545432538362538432545442539342538342545442538412542382545422541372541352545432538412541342545442539412538432545412542372538302e504e47)\n",
        "\n",
        "가설 : $H(X) = softmax(WX + B)$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Idz6bGMl__cE",
        "colab_type": "text"
      },
      "source": [
        "###4.2.2 소프트맥스 함수\n",
        "\n",
        "소프트맥스 함수는 클래스의 개수(k라고 하자)만큼의 차원을 가진 벡터를 입력받아 각 클래스에 대한 확률을 추정한다.\n",
        "\n",
        "1) 소프트맥스 함수의 이해\n",
        "\n",
        "k차원의 벡터에서 i번째 원소를 $z_i$, i번째 클래스가 정답일 확률을 $p_i$로 나타낸다고 하였을 때 소프트맥스 함수는 $p_i$를 다음과 같이 정의한다.\n",
        "\n",
        "$p_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{k} e^{z_{j}}}\\ \\ for\\ i=1, 2, ... k$\n",
        "\n",
        "![대체 텍스트](https://camo.githubusercontent.com/2be4df15c4c65877caf65dced6533ed34009e39e/68747470733a2f2f77696b69646f63732e6e65742f696d616765732f706167652f35393432372f2545432538362538432545442539342538342545442538412542382545422541372541352545432538412541342545442539412538432545412542372538302e504e47)\n",
        "\n",
        "위에서 풀어야하는 문제에 소프트맥스 함수를 차근차근 적용해보자. 문제의 경우 k=3이므로 3차원 벡터 $z=[z_{1}\\ z_{2}\\ z_{3}]$의 입력을 받으면 소프트맥스 함수는 아래와 같은 출력을 리턴한다.\n",
        "\n",
        "$softmax(z)=[\\frac{e^{z_{1}}}{\\sum_{j=1}^{3} e^{z_{j}}}\\ \\frac{e^{z_{2}}}{\\sum_{j=1}^{3} e^{z_{j}}}\\ \\frac{e^{z_{3}}}{\\sum_{j=1}^{3} e^{z_{j}}}] = [p_{1}, p_{2}, p_{3}] = \\hat{y} = \\text{예측값}$\n",
        "\n",
        "$p_1,p_2,p_3$  각각은 1번 클래스가 정답일 확률, 2번 클래스가 정답일 확률, 3번 클래스가 정답일 확률을 나타내며 각각 0과 1사이의 값으로 총 합은 1이 된다.\n",
        "\n",
        "여기서 분류하고자하는 3개의 클래스는 virginica, setosa, versicolor이므로 이는 결국 주어진 입력이 virginica일 확률, setosa일 확률, versicolor일 확률을 나타내는 값을 의미한다.\n",
        "\n",
        "여기서는 i가 1일 때는 virginica일 확률을 나타내고, 2일 때는 setosa일 확률, 3일때는 versicolor일 확률이라고 지정하였다고 하자. 이 지정 순서는 문제를 풀고자 하는 사람의 무작위 선택이다. 이에따라 식을 문제에 맞게 다시 쓰면 아래와 같다.\n",
        "\n",
        "$softmax(z)=[\\frac{e^{z_{1}}}{\\sum_{j=1}^{3} e^{z_{j}}}\\ \\frac{e^{z_{2}}}{\\sum_{j=1}^{3} e^{z_{j}}}\\ \\frac{e^{z_{3}}}{\\sum_{j=1}^{3} e^{z_{j}}}] = [p_{1}, p_{2}, p_{3}] = [p_{virginica}, p_{setosa}, p_{versicolor}]$\n",
        "\n",
        "정리 : 분류하고자 하는 클래스가 k개이면, k차원의 벡터를 입력받아서 벡터의 모든 원소의 값을 0과 1사이의 값으로 변경하고, 다시 k차원의 벡터를 리턴하면 된다.\n",
        "\n",
        "2) 그림을 통한 이해\n",
        "\n",
        "![](https://camo.githubusercontent.com/a6ede60782b3f8d590aca00e9e765ea5ec732afb/68747470733a2f2f77696b69646f63732e6e65742f696d616765732f706167652f33353437362f736f66746d6178315f66696e616c5f66696e616c2e504e47)\n",
        "\n",
        "질문 1 : 샘플 데이터를 소프트맥스 함수의 입력으로 어떻게 바꿀까?\n",
        "\n",
        "|SepalLengthCm$(x_1)$|SepalWidthCm$(x_2)$|PetalLengthCm$(x_3)$|PetalWidthCm$(x_4)$|Species$(y)$|\n",
        "|---|---|---|---|---|\n",
        "|5.1|3.5|1.4|0.2|setosa|\n",
        "|4.9|3.0|1.4|0.2|setosa|\n",
        "|5.8|2.6|4.0|1.2|versicolor|\n",
        "|6.7|3.0|5.2|2.3|virginica|\n",
        "|5.6|2.8|4.9|2.0|virginica|\n",
        "\n",
        "여기서 샘플 데이터를 1개씩 입력으로 받아 처리한다고 가정하자. 샘플 데이터가 1개씩 처리된다는 것은 배치 크기가 1이라는 것과 같은 의미다. 하나의 샘플 데이터는 4개의 독립 변수 $x$를 가지는데 이는 모델이 4차원 벡터를 입력으로 받음을 의미한다.\n",
        "\n",
        "4차원 벡터가 모델로 입력된 다음 단계는 소프트맥스 함수에 입력되는 것이다.\n",
        "\n",
        "모델에 입력된 벡터는 4차원이고, 소프트맥스 함수에 입력될 수 있는 벡터는 3차원이다.\n",
        "\n",
        "따라서, 입력된 4차원 벡터는 다시 어떤 가중치 연산을 통해 분류하고자 하는 클래스의 개수인 3차원 벡터로 변환되어야 한다. \n",
        "\n",
        "그래야 소프트맥스 함수의 입력으로 쓸 수 있다.\n",
        "\n",
        "아래 그림에서는 소프트맥스 함수의 입력으로 사용되는 3차원 벡터를 $z$로 표현했다.\n",
        "\n",
        "![](https://camo.githubusercontent.com/172a4d05ef2b15ec46c642dc195caf7fa97f047b/68747470733a2f2f77696b69646f63732e6e65742f696d616765732f706167652f33353437362f736f66746d61786265747765656e31616e64322e504e47)\n",
        "\n",
        "샘플 데이터 벡터(4차원)를 소프트맥스 함수의 입력 벡터(3차원)로 변환하는 방법은 소프트맥스 함수의 입력 벡터 $z$의 차원수만큼 결과값이 나오도록 가중치 곱을 진행하는 것이다.\n",
        "\n",
        "즉 (4 x 3 = 12) 12개의 전부 다른 가중치로 학습 과정에서 점차적으로 오차를 최소화하면 된다.\n",
        "\n",
        "질문 2 : 오차를 어떻게 구할까?\n",
        "\n",
        "여기서는 첫번째 원소인 $p_1$은 virginica가 정답일 확률, 두번째 원소인 $p_2$는 setosa가 정답일 확률, 세번째 원소인 $p_3$은 versicolor가 정답일 확률로 고려한다. 그렇다면 이 예측값과 비교를 할 수 있는 실제값의 표현 방법이 있어야 한다. 소프트맥스 회귀에서는 실제값을 원-핫 벡터로 표현한다. 아래 그림은 원-핫 인코딩을 수행하여 실제값을 원-핫 벡터로 수치화한 것을 보여준다.\n",
        "\n",
        "![](https://camo.githubusercontent.com/e57ce45ea8db9548650e711170568fd662b3d067/68747470733a2f2f77696b69646f63732e6e65742f696d616765732f706167652f33353437362f736f66746d6178325f66696e616c2e504e47)\n",
        "\n",
        "현재 풀고 있는 샘플 데이터의 실제값이 setosa라면 setosa의 원-핫 벡터는 [0,1,0]이다. 이 두 벡터의 오차를 계산하기 위해서 소프트맥스 회귀는 비용 함수로 크로스 엔트로피 함수를 사용한다.\n",
        "\n",
        "![](https://camo.githubusercontent.com/5af606fdacad2a5d610b760eaad071314945e153/68747470733a2f2f77696b69646f63732e6e65742f696d616765732f706167652f33353437362f736f66746d6178342e504e47)\n",
        "\n",
        "![](https://camo.githubusercontent.com/b07ec8f666e2c2e3490c7f6a0da57a3ad3d5701e/68747470733a2f2f77696b69646f63732e6e65742f696d616765732f706167652f33353437362f736f66746d6178352e504e47)\n",
        "\n",
        "![](https://camo.githubusercontent.com/92fb01a7238490aad23863f2b331ffdcc9ccba0b/68747470733a2f2f77696b69646f63732e6e65742f696d616765732f706167652f33353437362f736f66746d6178365f66696e616c2e504e47)\n",
        "\n",
        "이런식으로 앞서 배운 선형 회귀나 로지스틱 회귀와 마찬가지로 오차로부터 가중치를 업데이트 한다.\n",
        "\n",
        "소프트맥스 회구를 벡터와 행렬 연산으로 이해해보자. \n",
        "\n",
        "입력 : 특성(feature)의 수만큼의 차원을 가진 입력 벡터 $x$\n",
        "\n",
        "가중치 :  $W$\n",
        "\n",
        "편향 : $B$\n",
        "\n",
        "소프트맥스 회귀에서 예측값을 구하는 과정을 벡터와 행렬 연산으로 표현하면 아래와 같다.\n",
        "\n",
        "![](https://camo.githubusercontent.com/713e4d3501b73cb09e2d8ff1550ce3f08e719bc1/68747470733a2f2f77696b69646f63732e6e65742f696d616765732f706167652f35393432372f2545412542302538302545432538342541342e504e47)\n",
        "\n",
        "$f$는 특성의 수, $c$는 클래스의 개수이다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L9xn8tNaLm7f",
        "colab_type": "text"
      },
      "source": [
        "###4.2.3 붓꽃 품종 분류를 행렬 연산으로 이해하기\n",
        "\n",
        "|SepalLengthCm$(x_1)$|SepalWidthCm$(x_2)$|PetalLengthCm$(x_3)$|PetalWidthCm$(x_4)$|Species$(y)$|\n",
        "|---|---|---|---|---|\n",
        "|5.1|3.5|1.4|0.2|setosa|\n",
        "|4.9|3.0|1.4|0.2|setosa|\n",
        "|5.8|2.6|4.0|1.2|versicolor|\n",
        "|6.7|3.0|5.2|2.3|virginica|\n",
        "|5.6|2.8|4.9|2.0|virginica|\n",
        "\n",
        "우선 위 예제 데이터는 샘플의 수 5개, 특성의 수 4개로 5 x 4 행렬 $X$로 정의하자\n",
        "\n",
        "$X=\n",
        "\\left(\n",
        "    \\begin{array}{c}\n",
        "      5.1\\ 3.5\\ 1.4\\ 0.2\\ \\\\\n",
        "      4.9\\ 3.0\\ 1.4\\ 0.2\\ \\\\\n",
        "      5.8\\ 2.6\\ 4.0\\ 1.2\\ \\\\\n",
        "      6.7\\ 3.0\\ 5.2\\ 2.3\\ \\\\\n",
        "      5.6\\ 2.8\\ 4.9\\ 2.0\\ \\\\\n",
        "    \\end{array}\n",
        "  \\right)$\n",
        "\n",
        "각 행렬의 원소 위치를 반영한 변수로 표현하면\n",
        "\n",
        "$X=\\left(\n",
        "    \\begin{array}{c}\n",
        "      x_{11}\\ x_{12}\\ x_{13}\\ x_{14}\\ \\\\\n",
        "      x_{21}\\ x_{22}\\ x_{23}\\ x_{24}\\ \\\\\n",
        "      x_{31}\\ x_{32}\\ x_{33}\\ x_{34}\\ \\\\\n",
        "      x_{41}\\ x_{42}\\ x_{43}\\ x_{44}\\ \\\\\n",
        "      x_{51}\\ x_{52}\\ x_{53}\\ x_{54}\\ \\\\\n",
        "    \\end{array}\n",
        "  \\right)$\n",
        "\n",
        "  이 문제는 선택지가 총 3개인 문제이므로 (=클래스가 총 3개인 문제이므로) 가설의 예측값으로 얻는 행렬 $\\hat{Y}$의 열의 개수는 3개여야 한다. 각 행은 행렬 $X$의 각 행의 예측값이므로 행의 개수는 5개로 동일해야 한다. 행렬 $\\hat{Y}$의 크기는 5 × 3 이다.\n",
        "\n",
        "$\\hat{Y}=\\left(\n",
        "    \\begin{array}{c}\n",
        "      y_{11}\\ y_{12}\\ y_{13}\\ \\\\\n",
        "      y_{21}\\ y_{22}\\ y_{23}\\ \\\\\n",
        "      y_{31}\\ y_{32}\\ y_{33}\\ \\\\\n",
        "      y_{41}\\ y_{42}\\ y_{43}\\ \\\\\n",
        "      y_{51}\\ y_{52}\\ y_{53}\\ \\\\\n",
        "    \\end{array}\n",
        "  \\right)$\n",
        "\n",
        "크기 5 × 3의 행렬 $\\hat{Y}$는 \n",
        "\n",
        "크기 5 × 4 입력 행렬 $X$와 \n",
        "\n",
        "가중치 행렬 $W$의 곱으로 얻어지는 행렬이므로 \n",
        "\n",
        "가중치 행렬 $W$은 4 × 3의 크기를 가진 행렬임을 알 수 있다.\n",
        "\n",
        "$W=\\left(\n",
        "    \\begin{array}{c}\n",
        "      w_{11}\\ w_{12}\\ w_{13}\\ \\\\\n",
        "      w_{21}\\ w_{22}\\ w_{23}\\ \\\\\n",
        "      w_{31}\\ w_{32}\\ w_{33}\\ \\\\\n",
        "      w_{41}\\ w_{42}\\ w_{43}\\ \\\\\n",
        "    \\end{array}\n",
        "  \\right)$\n",
        "\n",
        "편향 행렬 $B$는 예측값 행렬 $\\hat{Y}$와 크기가 동일해야 하므로 5 × 3의 크기를 가진다.\n",
        "\n",
        "$B=\\left(\n",
        "    \\begin{array}{c}\n",
        "      b_{1}\\ b_{2}\\ b_{3}\\\\\n",
        "      b_{1}\\ b_{2}\\ b_{3}\\\\\n",
        "      b_{1}\\ b_{2}\\ b_{3}\\\\\n",
        "      b_{1}\\ b_{2}\\ b_{3}\\\\\n",
        "      b_{1}\\ b_{2}\\ b_{3}\\\\\n",
        "    \\end{array}\n",
        "  \\right)$\n",
        "\n",
        "결과적으로 가설식은 다음과 같다.\n",
        "\n",
        "$\\hat{Y} = softmax(XW + B)$\n",
        "\n",
        "$\\left(\n",
        "    \\begin{array}{c}\n",
        "      y_{11}\\ y_{12}\\ y_{13}\\ \\\\\n",
        "      y_{21}\\ y_{22}\\ y_{23}\\ \\\\\n",
        "      y_{31}\\ y_{32}\\ y_{33}\\ \\\\\n",
        "      y_{41}\\ y_{42}\\ y_{43}\\ \\\\\n",
        "      y_{51}\\ y_{52}\\ y_{53}\\ \\\\\n",
        "    \\end{array}\n",
        "  \\right)$=$softmax\\left(\n",
        "\\left(\n",
        "    \\begin{array}{c}\n",
        "      x_{11}\\ x_{12}\\ x_{13}\\ x_{14}\\ \\\\\n",
        "      x_{21}\\ x_{22}\\ x_{23}\\ x_{24}\\ \\\\\n",
        "      x_{31}\\ x_{32}\\ x_{33}\\ x_{34}\\ \\\\\n",
        "      x_{41}\\ x_{42}\\ x_{43}\\ x_{44}\\ \\\\\n",
        "      x_{51}\\ x_{52}\\ x_{53}\\ x_{54}\\ \\\\\n",
        "    \\end{array}\n",
        "  \\right)\n",
        "\\left(\n",
        "    \\begin{array}{c}\n",
        "      w_{11}\\ w_{12}\\ w_{13}\\ \\\\\n",
        "      w_{21}\\ w_{22}\\ w_{23}\\ \\\\\n",
        "      w_{31}\\ w_{32}\\ w_{33}\\ \\\\\n",
        "      w_{41}\\ w_{42}\\ w_{43}\\ \\\\\n",
        "    \\end{array}\n",
        "  \\right)\n",
        "+\n",
        "\\left(\n",
        "    \\begin{array}{c}\n",
        "      b_{1}\\ b_{2}\\ b_{3}\\\\\n",
        "      b_{1}\\ b_{2}\\ b_{3}\\\\\n",
        "      b_{1}\\ b_{2}\\ b_{3}\\\\\n",
        "      b_{1}\\ b_{2}\\ b_{3}\\\\\n",
        "      b_{1}\\ b_{2}\\ b_{3}\\\\\n",
        "    \\end{array}\n",
        "  \\right)\n",
        "\\right)$\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Co4EJyCWNXLP",
        "colab_type": "text"
      },
      "source": [
        "###4.2.4 비용 함수\n",
        "\n",
        "소프트맥스 회귀에서는 비용 함수로 크로스 엔트로피 함수를 사용한다.\n",
        "\n",
        "1) 크로스 엔트로피 함수\n",
        "\n",
        "$y$ : 실제값\n",
        "\n",
        "$k$ : 클래스의 개수\n",
        "\n",
        "$y_j$ : 실제값 원-핫 벡터의 $j$번째 인덱스를 의미\n",
        "\n",
        "$p_j$ : 샘플 데이터가 $j$번째 클래스일 확률 ($\\hat{y}_{j}$로 표현하기도 한다.)\n",
        "\n",
        "$cost(W) = -\\sum_{j=1}^{k}y_{j}\\ log(p_{j})$\n",
        "\n",
        "$c$가 실제값 원-핫 벡터에서 1을 가진 원소의 인덱스라고 한다면, $p_{c}=1$은 $\\hat{y}$가 $y$를 정확하게 예측한 경우가 된다.\n",
        "\n",
        "이를 식에 대입해보면 $-1 log(1) = 0$이 되기 때문에, 결과적으로 $\\hat{y}$가 $y$를 정확하게 예측한 경우의 크로스 엔트로피 함수의 값은 0이 된다. 즉, $-\\sum_{j=1}^{k}y_{j}\\ log(p_{j})$ 이 값을 최소화하는 방향으로 학습해야 한다.\n",
        "\n",
        "이제 이를 $n$개의 전체 데이터에 대한 평균을 구한다고 하면 최종 비용 함수는 다음과 같다.\n",
        "\n",
        "$cost(W) = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k}y_{j}^{(i)}\\ log(p_{j}^{(i)})$\n",
        "\n",
        "2) 이진 분류에서의 크로스 엔트로피 함수\n",
        "\n",
        "로지스틱 회귀에서 배운 크로스 엔트로피 함수식과 달라보이지만, 본질적으로는 동일한 함수식이다. 로지스틱 회귀의 크로스 엔트로피 함수식으로부터 소프트맥스 회귀의 크로스 엔트로피 함수식을 도출해보자.\n",
        "\n",
        "$cost(W) = -(y\\ logH(X) + (1-y)\\ log(1-H(X)))$\n",
        "\n",
        "위의 식은 앞서 로지스틱 회귀에서 배웠던 크로스 엔트로피의 함수식을 보여준다. 위의 식에서 $y$를 $y_1$, $y$−1을 $y_2$로 치환하고 $H(X)$를 $p_1$, 1−$H(X)$를 $p_2$로 치환하면 결과적으로 아래의 식을 얻을 수 있습니다.\n",
        "\n",
        "$-(y_{1}\\ log(p_{1})+y_{2}\\ log(p_{2}))$\n",
        "\n",
        "이 식은 아래와 같이 표현할 수 있다.\n",
        "\n",
        "$-(\\sum_{i=1}^{2}y_{i}\\ log\\ p_{i})$\n",
        "\n",
        "소프트맥스 회귀에서는 k의 값이 고정된 값이 아니므로 2를 k로 변경하면\n",
        "\n",
        "$-(\\sum_{i=1}^{k}y_{i}\\ log\\ p_{i})$\n",
        "\n",
        "위의 식은 결과적으로 소프트맥스 회귀의 식과 동일하다. 역으로 소프트맥스 회귀에서 로지스틱 회귀의 크로스 엔트로피 함수식을 얻는 것은 k를 2로 하고, $y_1$과 $y_2$를 각각 $y$와 1−$y$로 치환하고, $p_1$와 $p_2$를 각각 $H(X)$와 1−$H(X)$로 치환하면 된다.\n",
        "\n",
        "정리하면 소프트맥스 함수의 최종 비용 함수에서 $k$가 2라고 가정하면 결국 로지스틱 회귀의 비용 함수와 같다.\n",
        "\n",
        "$cost(W) = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k}y_{j}^{(i)}\\ log(p_{j}^{(i)}) = -\\frac{1}{n} \\sum_{i=1}^{n} [y^{(i)}log(p^{(i)}) + (1-y^{(i)})log(1-p^{(i)})]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dQXJtJPfPIkw",
        "colab_type": "text"
      },
      "source": [
        "##4.3 소프트맥스 회귀의 비용 함수 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uI00zK--j2s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "31ca1988-a17d-42de-f35a-36b18a364ac3"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "torch.manual_seed(1)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f3cffa5bcd0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EW99WqNFPf5C",
        "colab_type": "text"
      },
      "source": [
        "###4.3.1 파이토치로 소프트맥스 회귀의 비용 함수 구현하기 (로우-레벨)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pjt_quHwPqtp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "z = torch.FloatTensor([1,2,3])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYGaE_RzPwF2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "a9417d44-9324-4a23-8b99-984e225a5429"
      },
      "source": [
        "hypothesis = F.softmax(z, dim=0)\n",
        "print(hypothesis)"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0.0900, 0.2447, 0.6652])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "argQp_reP9l6",
        "colab_type": "text"
      },
      "source": [
        "$p_{i}=\\frac{e^{z_{i}}}{\\sum_{j=1}^{k} e^{z_{j}}}\\ \\ for\\ i=1, 2, ... k$\n",
        "\n",
        "$softmax(z)=[\\frac{e^{z_{1}}}{\\sum_{j=1}^{3} e^{z_{j}}}\\ \\frac{e^{z_{2}}}{\\sum_{j=1}^{3} e^{z_{j}}}\\ \\frac{e^{z_{3}}}{\\sum_{j=1}^{3} e^{z_{j}}}] = [p_{1}, p_{2}, p_{3}] = \\hat{y} = \\text{예측값}$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MiKU3KQcQJu4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2afe740e-8290-437b-ce86-b084f5411ba8"
      },
      "source": [
        "hypothesis.sum()"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2lFJ8YHQM-W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "fe168f12-66a7-4b91-f9ea-232062d1b7f9"
      },
      "source": [
        "z = torch.rand(3, 5, requires_grad=True)\n",
        "z"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7576, 0.2793, 0.4031, 0.7347, 0.0293],\n",
              "        [0.7999, 0.3971, 0.7544, 0.5695, 0.4388],\n",
              "        [0.6387, 0.5247, 0.6826, 0.3051, 0.4635]], requires_grad=True)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qWmHLo9tQS3p",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "63235359-e4ad-4dd0-b3a7-69fea9a25d74"
      },
      "source": [
        "hypothesis = F.softmax(z, dim=1) # 각 샘플에 대해 소프트맥스 함수를 적용해야 하므로, 두번째 차원에 대해 소프트맥스 함수를 적용한다는 의미로, dim=1\n",
        "print(hypothesis)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[0.2645, 0.1639, 0.1855, 0.2585, 0.1277],\n",
            "        [0.2430, 0.1624, 0.2322, 0.1930, 0.1694],\n",
            "        [0.2226, 0.1986, 0.2326, 0.1594, 0.1868]], grad_fn=<SoftmaxBackward>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJBth7-dQ1JF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "9693c073-5993-4fb1-fb76-6f110745880f"
      },
      "source": [
        "# 소프트맥스 함수의 출력값은 결국 예측값이고, 확률이다. 합은 1이다.\n",
        "print(hypothesis[0,:].sum())\n",
        "print(hypothesis[1,:].sum())\n",
        "print(hypothesis[2,:].sum())"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.0000, grad_fn=<SumBackward0>)\n",
            "tensor(1.0000, grad_fn=<SumBackward0>)\n",
            "tensor(1., grad_fn=<SumBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KWYXy9eYQlZo",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2765c6a5-6c51-4ee1-9eaf-1b049392b031"
      },
      "source": [
        "# 각 샘플에 대해 임의의 레이블을 만든다.\n",
        "y = torch.randint(5, (3,)).long()\n",
        "print(y)"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([0, 2, 1])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jFu-kiyoRkAe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "bed52351-2923-4a2f-c7f3-f4874d693c22"
      },
      "source": [
        "# 각 레이블에 대해 원-핫 인코딩을 수행한다.\n",
        "# 모든 원소가 0의 값을 가진 3 × 5 텐서 생성\n",
        "y_one_hot = torch.zeros_like(hypothesis) # hypothesis와 같은 크기를 가지고 모든 원소가 0의 값을 가지는 텐서를 만들어라.\n",
        "y_one_hot.scatter_(1, y.unsqueeze(1), 1)\n",
        "\"\"\"\n",
        "y.unsqueeze(1)에서 (3,)의 크기를 가졌던 y텐서는 (3x1) 텐서가 된다. \n",
        "scatter의 첫번째 인자는 dim=1 에 대해 수행하라는 의미\n",
        "세번째 인자에 1을 넣어줌으로서 두번째 인자인 y_unsqueeze(1)이 알려주는 위치에 숫자 1을 넣도록 한다.\n",
        "연산 뒤에 _를 붙이면 In-place Operation (덮어쓰기 연산) 이다.\n",
        "\"\"\"\n",
        "print(y_one_hot)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([[1., 0., 0., 0., 0.],\n",
            "        [0., 0., 1., 0., 0.],\n",
            "        [0., 1., 0., 0., 0.]])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJUboO4PRjZp",
        "colab_type": "text"
      },
      "source": [
        "$cost(W) = -\\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k}y_{j}^{(i)}\\ log(p_{j}^{(i)})$\n",
        "\n",
        "마이너스 부호를 뒤로 빼면 다음 식과도 동일하다.\n",
        "\n",
        "$cost(W) = \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{j=1}^{k}y_{j}^{(i)}\\ * (-log(p_{j}^{(i)}))$\n",
        "\n",
        "이를 코드로 구현하면 아래와 같다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OCAP66H8TibI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "6449eeb5-e5df-4151-db0a-8efd62ec92eb"
      },
      "source": [
        "cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
        "print(cost)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor(1.4689, grad_fn=<MeanBackward0>)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNtj6r9lPl1G",
        "colab_type": "text"
      },
      "source": [
        "###4.3.2 파이토치로 소프트맥스 회귀의 비용 함수 구현하기 (하이-레벨)\n",
        "\n",
        "1)  F.softmax() + torch.log() = F.log_softmax()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "83m5A86C0KIR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        },
        "outputId": "1e5a5767-576f-46c8-94d0-37fbf3250a9a"
      },
      "source": [
        "# Low level\n",
        "torch.log(F.softmax(z, dim=1))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
              "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
              "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]], grad_fn=<LogBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUCw8pSQT1in",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "b248e7c4-1abe-42ed-9d14-655b63628220"
      },
      "source": [
        "# High level\n",
        "F.log_softmax(z, dim=1)"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[-1.3301, -1.8084, -1.6846, -1.3530, -2.0584],\n",
              "        [-1.4147, -1.8174, -1.4602, -1.6450, -1.7758],\n",
              "        [-1.5025, -1.6165, -1.4586, -1.8360, -1.6776]],\n",
              "       grad_fn=<LogSoftmaxBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WSVIHOMLT6hV",
        "colab_type": "text"
      },
      "source": [
        "2) F.log_softmax() + F.nll_loss() = F.cross_entropy()"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGkmfU8wT2sv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2e59cfd8-7688-4b55-d6c0-ecd728868021"
      },
      "source": [
        "# Low level\n",
        "# 첫번째 수식\n",
        "(y_one_hot * -torch.log(F.softmax(z, dim=1))).sum(dim=1).mean()"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.4689, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jV87suPGT84k",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "50c15ba4-1ac8-40e8-d9a7-b61772c11273"
      },
      "source": [
        "# 두번째 수식\n",
        "(y_one_hot * - F.log_softmax(z, dim=1)).sum(dim=1).mean()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.4689, grad_fn=<MeanBackward0>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_zBUtKIpUCQm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "3153391b-11ae-4e3c-c1dd-d43f59e98c91"
      },
      "source": [
        "# High level\n",
        "# 세번째 수식\n",
        "F.nll_loss(F.log_softmax(z, dim=1), y)\n",
        "# nll : Negative Log Likelihood의 약자"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.4689, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVP6uhC4UQm2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "61025d3f-d762-48ac-8575-0ad781ac199f"
      },
      "source": [
        "# 네번째 수식\n",
        "F.cross_entropy(z, y)\n",
        "# 비용 함수에 소프트맥스 함수까지 포함하고 있음을 기억하고 있어야 구현 시 혼동하지 않는다."
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor(1.4689, grad_fn=<NllLossBackward>)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XnZtyablUjp5",
        "colab_type": "text"
      },
      "source": [
        "##4.4 소프트맥스 회귀 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_orNmGiUSVn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfq-YJKzUt2m",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "x_train : 8 x 4 \n",
        "y_trian : 8 x 1, 3개의 클래스\n",
        "\"\"\"\n",
        "\n",
        "x_train = [[1, 2, 1, 1],\n",
        "           [2, 1, 3, 2],\n",
        "           [3, 1, 3, 4],\n",
        "           [4, 1, 5, 5],\n",
        "           [1, 7, 5, 5],\n",
        "           [1, 2, 5, 6],\n",
        "           [1, 6, 6, 6],\n",
        "           [1, 7, 7, 7]]\n",
        "y_train = [2, 2, 2, 1, 1, 1, 0, 0]\n",
        "x_train = torch.FloatTensor(x_train)\n",
        "y_train = torch.LongTensor(y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ktlGJJvCVGpg",
        "colab_type": "text"
      },
      "source": [
        "###4.4.1 소프트맥스 회귀 구현하기(로우-레벨)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4in7IZTiVBZO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "b4ff62b6-dfb5-4a11-fd49-230b97580845"
      },
      "source": [
        "print(x_train.shape)\n",
        "print(y_train.shape)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 4])\n",
            "torch.Size([8])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MXNE_EnNVNr1",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "54e355fe-13cb-40a7-c5fa-e8d7ed054319"
      },
      "source": [
        "# y_train은 원-핫 인코딩 시켜줘야 한다.\n",
        "y_one_hot = torch.zeros(8, 3)\n",
        "y_one_hot.scatter_(1, y_train.unsqueeze(1), 1)\n",
        "print(y_one_hot.shape)"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([8, 3])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQGXHt12VZPf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# X가 8x4, Y가 8x3이므로 W는 4x3임을 추정할 수 있다.\n",
        "\n",
        "# 모델 초기화\n",
        "W = torch.zeros((4, 3), requires_grad=True)\n",
        "b = torch.zeros(3, requires_grad=True) # 선형 회귀, 로지스틱 회귀에서는 b가 1이지만 이제는 클래스 개수 만큼이다.\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtBcfE55VpPG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "4e44b884-537e-4156-823f-41d0ec4835f7"
      },
      "source": [
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # 가설\n",
        "    hypothesis = F.softmax(x_train.matmul(W) + b, dim=1) \n",
        "\n",
        "    # 비용 함수\n",
        "    cost = (y_one_hot * -torch.log(hypothesis)).sum(dim=1).mean()\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}/{nb_epochs}, Cost: {cost.item()}')"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/1000, Cost: 1.0986123085021973\n",
            "Epoch 100/1000, Cost: 0.704199492931366\n",
            "Epoch 200/1000, Cost: 0.6229994893074036\n",
            "Epoch 300/1000, Cost: 0.5657168030738831\n",
            "Epoch 400/1000, Cost: 0.5152913928031921\n",
            "Epoch 500/1000, Cost: 0.467661589384079\n",
            "Epoch 600/1000, Cost: 0.42127782106399536\n",
            "Epoch 700/1000, Cost: 0.37540143728256226\n",
            "Epoch 800/1000, Cost: 0.3297658860683441\n",
            "Epoch 900/1000, Cost: 0.2850724756717682\n",
            "Epoch 1000/1000, Cost: 0.2481546401977539\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NIfJ7wHV8II",
        "colab_type": "text"
      },
      "source": [
        "###4.4.2 소프트맥스 회귀 구현하기(하이-레벨)\n",
        "\n",
        "주의할 점은 F.cross_entropy()는 그 자체로 소프트맥스 함수를 포함하고 있다는 것이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AyESSTi7V5bm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델 초기화\n",
        "W = torch.zeros((4, 3), requires_grad=True)\n",
        "b = torch.zeros(3, requires_grad=True) # 선형 회귀, 로지스틱 회귀에서는 b가 1이지만 이제는 클래스 개수 만큼이다.\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD([W, b], lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "He-e8kFbWMEX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "e5bea580-895d-431e-a2b6-54dfb62fc737"
      },
      "source": [
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # Cost 계산\n",
        "    z = x_train.matmul(W) + b\n",
        "    cost = F.cross_entropy(z, y_train)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}/{nb_epochs}, Cost: {cost.item()}')"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/1000, Cost: 1.0986123085021973\n",
            "Epoch 100/1000, Cost: 0.7041994333267212\n",
            "Epoch 200/1000, Cost: 0.6229996681213379\n",
            "Epoch 300/1000, Cost: 0.5657167434692383\n",
            "Epoch 400/1000, Cost: 0.5152913928031921\n",
            "Epoch 500/1000, Cost: 0.46766164898872375\n",
            "Epoch 600/1000, Cost: 0.421277791261673\n",
            "Epoch 700/1000, Cost: 0.37540167570114136\n",
            "Epoch 800/1000, Cost: 0.3297656178474426\n",
            "Epoch 900/1000, Cost: 0.28507253527641296\n",
            "Epoch 1000/1000, Cost: 0.2481546550989151\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZwBr8ycWduP",
        "colab_type": "text"
      },
      "source": [
        "###4.4.3 소프트맥스 회귀 nn.Module로 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HOxfA4YtWbdP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 모델을 선언 및 초기화. 4개의 특성을 가지고 3개의 클래스로 분류. input_dim=4, output_dim=3.\n",
        "model = nn.Linear(4, 3)\n",
        "# optimizer 설정\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z5PS9sdrWpr4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "f2107e79-888c-4634-afa5-40af0a872524"
      },
      "source": [
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train) # F.cross_entropy() 가 소프트맥스 함수를 포함하고 있어서 가설에 정의하지 않음\n",
        "    \n",
        "    # Cost 계산\n",
        "    cost = F.cross_entropy(prediction, y_train)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}/{nb_epochs}, Cost: {cost.item()}')"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/1000, Cost: 1.8495128154754639\n",
            "Epoch 100/1000, Cost: 0.6898943781852722\n",
            "Epoch 200/1000, Cost: 0.6092584133148193\n",
            "Epoch 300/1000, Cost: 0.5512182116508484\n",
            "Epoch 400/1000, Cost: 0.5001410841941833\n",
            "Epoch 500/1000, Cost: 0.4519471228122711\n",
            "Epoch 600/1000, Cost: 0.405051052570343\n",
            "Epoch 700/1000, Cost: 0.35873302817344666\n",
            "Epoch 800/1000, Cost: 0.31291159987449646\n",
            "Epoch 900/1000, Cost: 0.26952165365219116\n",
            "Epoch 1000/1000, Cost: 0.2419215738773346\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4by5SSf_XQFw",
        "colab_type": "text"
      },
      "source": [
        "###4.4.4 소프트맥스 회귀 클래스로 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PA2IhGF7XITJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxClassifierModel(nn.Module):\n",
        "  def __init__(self):\n",
        "    super().__init__()\n",
        "    self.linear = nn.Linear(4, 3)\n",
        "\n",
        "  def forward(self, x):\n",
        "    return self.linear(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FzI-ZfX6Xjia",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SoftmaxClassifierModel()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k0GhbOHEXrpi",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "a8415bee-219d-4481-ad45-4129f956def1"
      },
      "source": [
        "nb_epochs = 1000\n",
        "for epoch in range(nb_epochs + 1):\n",
        "\n",
        "    # H(x) 계산\n",
        "    prediction = model(x_train) # F.cross_entropy() 가 소프트맥스 함수를 포함하고 있어서 가설에 정의하지 않음\n",
        "    \n",
        "    # Cost 계산\n",
        "    cost = F.cross_entropy(prediction, y_train)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # 100번마다 로그 출력\n",
        "    if epoch % 100 == 0:\n",
        "        print(f'Epoch {epoch}/{nb_epochs}, Cost: {cost.item()}')"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 0/1000, Cost: 1.845719814300537\n",
            "Epoch 100/1000, Cost: 0.6471500396728516\n",
            "Epoch 200/1000, Cost: 0.5688682198524475\n",
            "Epoch 300/1000, Cost: 0.5156992077827454\n",
            "Epoch 400/1000, Cost: 0.4717271327972412\n",
            "Epoch 500/1000, Cost: 0.43248626589775085\n",
            "Epoch 600/1000, Cost: 0.39587926864624023\n",
            "Epoch 700/1000, Cost: 0.36050641536712646\n",
            "Epoch 800/1000, Cost: 0.32522740960121155\n",
            "Epoch 900/1000, Cost: 0.2892172932624817\n",
            "Epoch 1000/1000, Cost: 0.2540856897830963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROyukvSwYIMB",
        "colab_type": "text"
      },
      "source": [
        "##4.5 소프트맥스 회귀로 MNIST 데이터 분류하기\n",
        "\n",
        "MNIST 데이터는 아래의 링크에 공개되어져 있다.\n",
        "\n",
        "링크 : http://yann.lecun.com/exdb/mnist"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MO5iMCbRYTQB",
        "colab_type": "text"
      },
      "source": [
        "###4.5.1 MNIST 데이터 이해하기\n",
        "\n",
        "![](https://camo.githubusercontent.com/06d848083b3f0d576769c6117934da8671de378b/68747470733a2f2f77696b69646f63732e6e65742f696d616765732f706167652f36303332342f6d6e6973742e706e67)\n",
        "\n",
        "MNIST는 숫자 0부터 9까지의 이미지로 구성된 손글씨 데이터셋이다. 이 데이터는 과거에 우체국에서 편지의 우편 번호를 인식하기 위해서 만들어진 훈련 데이터이다. 총 60,000개의 훈련 데이터와 레이블, 총 10,000개의 테스트 데이터와 레이블로 구성되어져 있다. 레이블은 0부터 9까지 총 10개다. 이 예제는 머신 러닝을 처음 배울 때 접하게 되는 가장 기본적인 예제다.\n",
        "\n",
        "MNIST 문제는 손글씨로 적힌 숫자 이미지가 들어오면, 그 이미지가 무슨 숫자인지 맞추는 문제다. 예를 들어 숫자 5의 이미지가 입력으로 들어오면 이게 숫자 5다! 라는 것을 맞춰야 한다. 이 문제는 사람에게는 굉장히 간단하지만 기계에게는 그렇지가 않다.\n",
        "\n",
        "우선 MNIST 문제를 더 자세히 보자. 각각의 이미지는 아래와 같이 28 픽셀 × 28 픽셀의 이미지다.\n",
        "\n",
        "![](https://camo.githubusercontent.com/e49526e694f6e69b6b2eda80fa919c20d9acbebe/68747470733a2f2f77696b69646f63732e6e65742f696d616765732f706167652f36303332342f6d6e6973745f535662635959472e706e67)\n",
        "\n",
        "이 문제를 풀기 위해 여기서는 28 픽셀 × 28 픽셀 = 784 픽셀이므로, 각 이미지를 총 784의 원소를 가진 벡터로 만들어줄거다. 이렇게 되면 총 784개의 특성을 가진 샘플이 되는데, 이는 앞서 우리가 풀었던 그 어떤 문제들보다 특성이 굉장히 많은 샘플이 된다.\n",
        "\n",
        "![](https://camo.githubusercontent.com/d6beaf8cdde9490a7129db3cf0ecfc2df964c665/68747470733a2f2f77696b69646f63732e6e65742f696d616765732f706167652f36303332342f2545422538422541342545432539412542342545422541312539432545422539332539432e706e67)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J6i1LlwRXvjg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchvision.datasets as dsets\n",
        "import torchvision.transforms as transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import random"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fC2mgDiIZIy7",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 379,
          "referenced_widgets": [
            "efeff0032f414097914fbf733f36f4a2",
            "8592d4b9e9d743edb02108a9b64f2127",
            "8e0f7804b0484ea3a880963018811b5f",
            "107e434374284679a7f716aee707c923",
            "ac471eb8021d46c4878db59005d7a820",
            "0f0f295d198541998270da5f06fe863e",
            "442274cfa52541d79571860f6ffe1173",
            "6df0311cd47d46c2a1ef75285ff448d5",
            "9970a07a2b7d4303a22010d9a7573ea9",
            "0dad6fe3bbc8472d947f733638d1b44b",
            "e771f7fa59bf485db7bb5b1dbd99a903",
            "2114cadc761d46e5a1c26a3a86197fbb",
            "ed85ab0e4f9e46b68d7b60f8618c4f9a",
            "afe956f603754c04a8319479bccd9d3d",
            "4939cb5328f545289d413535e897e779",
            "671e0c95b1f2467285770a27fe279bf8",
            "77376caabc3d41b9a2684cde6d6766fd",
            "d177bdfa11ef47ecb1b08f9ad6ac1b19",
            "e1bbec8dc2af40ba909e080664866887",
            "b73236930af24b84a234ddb835273a97",
            "2eb3dfdfce94432b916e35b2cb2664fb",
            "69d5425e1ea648368e3dd6428b9b5311",
            "e19aa26f87db411f8c2004ddbba897f2",
            "22a19dbf8d4b4cf78172985263cc9ab7",
            "00a94c98892946b4979b4f17eb7529a9",
            "94726422a2cb4ca290b0a62629f26b83",
            "9232147ae52849f0afbb811dcde50140",
            "44ea248dfc67413ea30697ccef78dd57",
            "02868d2b879b4204af0d7b7087ab05a0",
            "5c222876aafa4949a06fb1df51f01506",
            "d93a27f9a70f487c98bad5ff87e9f63b",
            "85e6dc73bf6c422da011d4ecb230249c"
          ]
        },
        "outputId": "7a80497d-9dbb-41d2-fd9c-985864fae254"
      },
      "source": [
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "efeff0032f414097914fbf733f36f4a2",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9970a07a2b7d4303a22010d9a7573ea9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/train-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "77376caabc3d41b9a2684cde6d6766fd",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-images-idx3-ubyte.gz to MNIST_data/MNIST/raw\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "00a94c98892946b4979b4f17eb7529a9",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=1, bar_style='info', max=1), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Extracting MNIST_data/MNIST/raw/t10k-labels-idx1-ubyte.gz to MNIST_data/MNIST/raw\n",
            "Processing...\n",
            "Done!\n",
            "\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ysf_dbCZL0o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "055cfc76-5733-407d-b2da-39f3c875e0d5"
      },
      "source": [
        "X,y = mnist_train[0]\n",
        "print('X size = ',X.size())\n",
        "print('입력 이미지를 [batch_size × 784]의 크기로 reshape 하면 X size = ',X.view(-1, 28*28).size())"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "X size =  torch.Size([1, 28, 28])\n",
            "입력 이미지를 [batch_size × 784]의 크기로 reshape 하면 X size =  torch.Size([1, 784])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUZ2P8KOZQWp",
        "colab_type": "text"
      },
      "source": [
        "### 4.5.2 토치비전(torchvision) 소개하기\n",
        "\n",
        "본격적인 실습에 들어가기에 앞서 토치비전(torchvision)이라는 도구를 설명하겠다. torchvision은 유명한 데이터셋들, 이미 구현되어져 있는 유명한 모델들, 일반적인 이미지 전처리 도구들을 포함하고 있는 패키지다. 아래의 링크는 torchvision에 어떤 데이터셋들(datasets)과 모델들(models) 그리고 어떤 전처리 방법들(transforms)을 제공하고 있는지 보여준다.\n",
        "\n",
        "링크 : https://pytorch.org/docs/stable/torchvision/index.html\n",
        "\n",
        "자연어 처리를 위해서는 토치텍스트(torchtext)라는 패키지가 있다.\n",
        "\n",
        "안 볼거다 ㅎ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rv5Afu_fZfrA",
        "colab_type": "text"
      },
      "source": [
        "### 4.5.3 분류기 구현을 위한 사전 설정"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zp23N7TgZO84",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "04d44718-7210-47a3-da6f-2b8856a79d06"
      },
      "source": [
        "USE_CUDA = torch.cuda.is_available() # GPU를 사용가능하면 True, 아니라면 False를 리턴\n",
        "device = torch.device(\"cuda\" if USE_CUDA else \"cpu\") # GPU 사용 가능하면 사용하고 아니면 CPU 사용\n",
        "print(\"다음 기기로 학습합니다:\", device)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "다음 기기로 학습합니다: cuda\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O47xEv2oZ09j",
        "colab_type": "text"
      },
      "source": [
        "구글의 Colab에서 '런타임 > 런타임 유형 변경 > 하드웨어 가속기 > GPU'를 선택하면 USE_CUDA의 값이 True가 되면서 '다음 기기로 학습합니다: cuda'라는 출력이 나온다.\n",
        "\n",
        "즉, GPU로 연산하겠다는 의미다. 반면에 '하드웨어 가속기 > None'을 선택하면 USE_CUDA의 값이 False가 되면서 '다음 기기로 학습합니다: cpu'라는 출력이 나온다. 즉, CPU로 연산하겠다는 의미다.\n",
        "\n",
        "위의 방법은 앞으로 자주 쓰이게되므로 기억하자.\n",
        "\n",
        "랜덤 시드를 고정하자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31sjJPbjZroM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# for reproducibility\n",
        "random.seed(777)\n",
        "torch.manual_seed(777)\n",
        "if device == 'cuda':\n",
        "    torch.cuda.manual_seed_all(777)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "10LnFeEYaFw1",
        "colab_type": "text"
      },
      "source": [
        "### 4.5.4 MNIST 분류기 구현하기"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qRn5WiNqaCiQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MNIST dataset\n",
        "mnist_train = dsets.MNIST(root='MNIST_data/',\n",
        "                          train=True,\n",
        "                          transform=transforms.ToTensor(),\n",
        "                          download=True)\n",
        "\n",
        "mnist_test = dsets.MNIST(root='MNIST_data/',\n",
        "                         train=False,\n",
        "                         transform=transforms.ToTensor(),\n",
        "                         download=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KknR51Ieb99v",
        "colab_type": "text"
      },
      "source": [
        "첫번째 인자 root : MNIST 데이터를 다운로드 받을 경로\n",
        "\n",
        "두번째 인자 train : True를 주면 MNIST의 훈련 데이터를 리턴, False를 주면 테스트 데이터를 리턴\n",
        "\n",
        "세번째 인자 transform : 현재 데이터를 파이토치 텐서로 변환\n",
        "\n",
        "네번째 인자 download : 해당 경로에 MNIST 데이터가 없다면 다운로드 받겠다는 의미\n",
        "\n",
        "이렇게 데이터를 다운로드했다면 앞서 미니 배치와 데이터로드 챕터에서 학습했던 데이터로더(DataLoader)를 사용하자."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aXX_kadJqmnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# hyperparameters를 변수로 두고 시작한다.\n",
        "training_epochs = 15 # nb_epochs의 기능과 같다.\n",
        "batch_size = 100"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Mxa4NQhb6Yc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# dataset loader\n",
        "data_loader = DataLoader(dataset=mnist_train,\n",
        "                                          batch_size=batch_size, # 배치 크기는 100\n",
        "                                          shuffle=True,\n",
        "                                          drop_last=True) # drop_last는 데이터를 배치 크기에 맞게 나눈 후(여기서는 100씩) 나머지(여기서는 100보다 작은 수)를 버린다는 의미다."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fmqt7vM4ccIz",
        "colab_type": "text"
      },
      "source": [
        "이때 DataLoader에는 4개의 인자가 있다.\n",
        "\n",
        "첫번째 인자 DataLoader : 로드할 대상을 의미\n",
        "\n",
        "두번째 인자 batch_size : 배치 크기\n",
        "\n",
        "세번째 인자 shuffle : 매 에포크마다 미니 배치를 셔플할 것인지의 여부\n",
        "\n",
        "네번째 인자 drop_last : 마지막 배치를 버릴 것인지를 의미합니다.\n",
        "\n",
        "drop_last를 하는 이유 : 예를 들어, 1,000개의 데이터가 있다고 했을 때 배치 크기가 128이라고 해보자. 1,000을 128로 나누면 총 7개가 나오고 나머지로 104개가 남는다. 이때 104개를 마지막 배치로 한다고 하였을 때 128개를 충족하지 못하였으므로 104개를 그냥 버릴 수도 있다. 이때 마지막 배치를 버리려면 drop_last=True를 해주면 된다. 이는 다른 미니 배치보다 개수가 적은 마지막 배치를 경사 하강법에 사용하여 마지막 배치가 상대적으로 과대 평가되는 현상을 막아준다.\n",
        "\n",
        "이제 모델을 설계하자. input_dim은 784이고, output_dim은 10이다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KUVTn9pKdX18",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# MNIST data image of shape 28 * 28 = 784\n",
        "model = nn.Linear(784, 10, bias=True).to(device)\n",
        "# 비용 함수와 옵티마이저 정의\n",
        "criterion = nn.CrossEntropyLoss().to(device) # 내부적으로 소프트맥스 함수를 포함하고 있음.\n",
        "# 크라이티어리언, 표준이라는 뜻, 굳이 안써도 되는데 왜쓰는지 모르겠다.\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dwghsMwBdgCo",
        "colab_type": "text"
      },
      "source": [
        "to() 함수는 연산을 어디서 수행할지를 정한다. \n",
        "\n",
        "to() 함수는 모델의 매개변수를 지정한 장치의 메모리로 보낸다. \n",
        "\n",
        "CPU를 사용할 경우에는 필요가 없지만, GPU를 사용하려면 to('cuda')를 해 줄 필요가 있다. \n",
        "\n",
        "아무것도 지정하지 않은 경우에는 CPU 연산이라고 보면 된다.\n",
        "\n",
        "bias는 편향 b를 사용할 것인지를 나타낸다. 기본값은 True이므로 굳이 할 필요는 없지만 명시적으로 True를 해주었다.\n",
        "\n",
        "앞서 소프트맥스 회귀를 배울 때는 torch.nn.functional.cross_entropy()를 사용하였으나 여기서는 torch.nn.CrossEntropyLoss()을 사용하고 있다. \n",
        "\n",
        "둘 다 파이토치에서 제공하는 크로스 엔트로피 함수로 소프트맥스 함수를 포함하고 있다."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vhUWXpljdv8i",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 287
        },
        "outputId": "a59c0e3e-5da5-46a1-9203-86c6c712e208"
      },
      "source": [
        "for epoch in range(training_epochs): # 앞서 training_epochs의 값은 15로 지정함.\n",
        "  avg_cost = 0 \n",
        "  \"\"\"\n",
        "  나는 한 epoch 당 cost를 알고 싶다.(60,000개로 한 바퀴 돌렸을 때의 cost) \n",
        "  근데 여기서 cost는 batch 마다 나온다. (한 배치인 100개를 돌렸을 때의 cost) \n",
        "  그래서 각 배치에서의 cost를 모두 더 한후 batch의 수 만큼 나눠줄거다.\n",
        "  \"\"\"\n",
        "  num_batch = len(data_loader) # 60,000개를 100개씩 나누었으니 600개의 batch가 생긴다.\n",
        "    \n",
        "  for X, Y in data_loader:\n",
        "    # 배치 크기가 100이므로 아래의 연산에서 X는 (100, 784)의 텐서가 된다.\n",
        "    X = X.view(-1, 28 * 28).to(device)\n",
        "    # 레이블은 원-핫 인코딩이 된 상태가 아니라 0 ~ 9의 정수.\n",
        "    Y = Y.to(device)\n",
        "    \n",
        "    # H(x) 계산\n",
        "    hypothesis = model(X)\n",
        "\n",
        "    # Cost 계산\n",
        "    cost = criterion(hypothesis, Y)\n",
        "\n",
        "    # cost로 H(x) 개선\n",
        "    optimizer.zero_grad()\n",
        "    cost.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    # print(f'cost : {cost}')\n",
        "    avg_cost += cost / num_batch\n",
        "\n",
        "  print(f'Epoch : {epoch}, Cost : {avg_cost}')"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 0, Cost : 0.5354684591293335\n",
            "Epoch : 1, Cost : 0.3592742085456848\n",
            "Epoch : 2, Cost : 0.33118751645088196\n",
            "Epoch : 3, Cost : 0.31657806038856506\n",
            "Epoch : 4, Cost : 0.30715814232826233\n",
            "Epoch : 5, Cost : 0.3001807630062103\n",
            "Epoch : 6, Cost : 0.29513019323349\n",
            "Epoch : 7, Cost : 0.2908514738082886\n",
            "Epoch : 8, Cost : 0.28741705417633057\n",
            "Epoch : 9, Cost : 0.2843795716762543\n",
            "Epoch : 10, Cost : 0.2818252742290497\n",
            "Epoch : 11, Cost : 0.2798007130622864\n",
            "Epoch : 12, Cost : 0.2778089940547943\n",
            "Epoch : 13, Cost : 0.2761543393135071\n",
            "Epoch : 14, Cost : 0.2744408845901489\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fNbXhUEyecM9",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        },
        "outputId": "229456e9-618b-4894-e60f-464ad5e38fe4"
      },
      "source": [
        "# 테스트 데이터를 사용하여 모델을 테스트한다.\n",
        "with torch.no_grad(): # torch.no_grad()를 하면 gradient 계산을 수행하지 않는다.\n",
        "    X_test = mnist_test.test_data.view(-1, 28 * 28).float().to(device)\n",
        "    Y_test = mnist_test.test_labels.to(device)\n",
        "\n",
        "    prediction = model(X_test)\n",
        "    correct_prediction = torch.argmax(prediction, 1) == Y_test\n",
        "    accuracy = correct_prediction.float().mean()\n",
        "    print('Accuracy:', accuracy.item())\n",
        "\n",
        "    # MNIST 테스트 데이터에서 무작위로 하나를 뽑아서 예측을 해본다\n",
        "    r = random.randint(0, len(mnist_test) - 1)\n",
        "    X_single_data = mnist_test.test_data[r:r + 1].view(-1, 28 * 28).float().to(device)\n",
        "    Y_single_data = mnist_test.test_labels[r:r + 1].to(device)\n",
        "\n",
        "    print('Label: ', Y_single_data.item())\n",
        "    single_prediction = model(X_single_data)\n",
        "    print('Prediction: ', torch.argmax(single_prediction, 1).item())\n",
        "\n",
        "    plt.imshow(mnist_test.test_data[r:r + 1].view(28, 28), cmap='Greys', interpolation='nearest')\n",
        "    plt.show()"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy: 0.8862999677658081\n",
            "Label:  8\n",
            "Prediction:  3\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:60: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py:50: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ux48RIQbem2W",
        "colab_type": "text"
      },
      "source": [
        "torch.no_grad() 와 model.eval()\n",
        "\n",
        "공통점 : 범위 안에서는 gradient 계산을 하지 않는다.\n",
        "\n",
        "차이점 : torch.no_grad()는 해당 범위 안에서 gradient 계산을 중지시킴으로써 메모리 사용량을 줄이고 계산 속도를 빨리 한다.\n",
        "\n",
        "model.train()과 model.eval()은 모델이 학습 모드인지, 테스트 모드인지를 정하는 것이다. dropout이나 batchnorm이 있는 모델의 경우 학습할 때와 테스트할 때 모델이 달라지기 때문에 세팅한다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7u_kuckT0Lyo",
        "colab_type": "text"
      },
      "source": [
        "##4.6 활용"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ExrrunF70Pxf",
        "colab_type": "text"
      },
      "source": [
        "출처 : https://github.com/Namsik-Yoon/pytorch_basic/blob/master/4_1_%EC%86%8C%ED%94%84%ED%8A%B8%EB%A7%A5%EC%8A%A4_%ED%9A%8C%EA%B7%80_with_My_data.ipynb\n",
        "\n",
        "sklearn에서 제공하는 미국 삼림 수종 데이터를 바탕으로 소프트맥스 회귀모델 학습"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vhji6hyH0OZa",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "1c30dd22-1da5-4c02-8b83-b466e18c7455"
      },
      "source": [
        "import pandas as pd\n",
        "from sklearn.datasets import fetch_covtype\n",
        "\n",
        "covtype = fetch_covtype()"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading https://ndownloader.figshare.com/files/5976039\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zbhLX8KU0ZeP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 539
        },
        "outputId": "8924b541-e520-4b1d-a174-cb27fbc06fad"
      },
      "source": [
        "print(covtype.DESCR)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            ".. _covtype_dataset:\n",
            "\n",
            "Forest covertypes\n",
            "-----------------\n",
            "\n",
            "The samples in this dataset correspond to 30×30m patches of forest in the US,\n",
            "collected for the task of predicting each patch's cover type,\n",
            "i.e. the dominant species of tree.\n",
            "There are seven covertypes, making this a multiclass classification problem.\n",
            "Each sample has 54 features, described on the\n",
            "`dataset's homepage <https://archive.ics.uci.edu/ml/datasets/Covertype>`__.\n",
            "Some of the features are boolean indicators,\n",
            "while others are discrete or continuous measurements.\n",
            "\n",
            "**Data Set Characteristics:**\n",
            "\n",
            "    =================   ============\n",
            "    Classes                        7\n",
            "    Samples total             581012\n",
            "    Dimensionality                54\n",
            "    Features                     int\n",
            "    =================   ============\n",
            "\n",
            ":func:`sklearn.datasets.fetch_covtype` will load the covertype dataset;\n",
            "it returns a dictionary-like object\n",
            "with the feature matrix in the ``data`` member\n",
            "and the target values in ``target``.\n",
            "The dataset will be downloaded from the web if necessary.\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LrZEWflD0bIu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "6502957e-a3d3-44d5-fbe4-80de65314eed"
      },
      "source": [
        "covtype_df = pd.DataFrame(data=covtype['data'])\n",
        "covtype_df"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2596.0</td>\n",
              "      <td>51.0</td>\n",
              "      <td>3.0</td>\n",
              "      <td>258.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>510.0</td>\n",
              "      <td>221.0</td>\n",
              "      <td>232.0</td>\n",
              "      <td>148.0</td>\n",
              "      <td>6279.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2590.0</td>\n",
              "      <td>56.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>212.0</td>\n",
              "      <td>-6.0</td>\n",
              "      <td>390.0</td>\n",
              "      <td>220.0</td>\n",
              "      <td>235.0</td>\n",
              "      <td>151.0</td>\n",
              "      <td>6225.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2804.0</td>\n",
              "      <td>139.0</td>\n",
              "      <td>9.0</td>\n",
              "      <td>268.0</td>\n",
              "      <td>65.0</td>\n",
              "      <td>3180.0</td>\n",
              "      <td>234.0</td>\n",
              "      <td>238.0</td>\n",
              "      <td>135.0</td>\n",
              "      <td>6121.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>2785.0</td>\n",
              "      <td>155.0</td>\n",
              "      <td>18.0</td>\n",
              "      <td>242.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>3090.0</td>\n",
              "      <td>238.0</td>\n",
              "      <td>238.0</td>\n",
              "      <td>122.0</td>\n",
              "      <td>6211.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>2595.0</td>\n",
              "      <td>45.0</td>\n",
              "      <td>2.0</td>\n",
              "      <td>153.0</td>\n",
              "      <td>-1.0</td>\n",
              "      <td>391.0</td>\n",
              "      <td>220.0</td>\n",
              "      <td>234.0</td>\n",
              "      <td>150.0</td>\n",
              "      <td>6172.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581007</th>\n",
              "      <td>2396.0</td>\n",
              "      <td>153.0</td>\n",
              "      <td>20.0</td>\n",
              "      <td>85.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>108.0</td>\n",
              "      <td>240.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>118.0</td>\n",
              "      <td>837.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581008</th>\n",
              "      <td>2391.0</td>\n",
              "      <td>152.0</td>\n",
              "      <td>19.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>12.0</td>\n",
              "      <td>95.0</td>\n",
              "      <td>240.0</td>\n",
              "      <td>237.0</td>\n",
              "      <td>119.0</td>\n",
              "      <td>845.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581009</th>\n",
              "      <td>2386.0</td>\n",
              "      <td>159.0</td>\n",
              "      <td>17.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>7.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>236.0</td>\n",
              "      <td>241.0</td>\n",
              "      <td>130.0</td>\n",
              "      <td>854.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581010</th>\n",
              "      <td>2384.0</td>\n",
              "      <td>170.0</td>\n",
              "      <td>15.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>5.0</td>\n",
              "      <td>90.0</td>\n",
              "      <td>230.0</td>\n",
              "      <td>245.0</td>\n",
              "      <td>143.0</td>\n",
              "      <td>864.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581011</th>\n",
              "      <td>2383.0</td>\n",
              "      <td>165.0</td>\n",
              "      <td>13.0</td>\n",
              "      <td>60.0</td>\n",
              "      <td>4.0</td>\n",
              "      <td>67.0</td>\n",
              "      <td>231.0</td>\n",
              "      <td>244.0</td>\n",
              "      <td>141.0</td>\n",
              "      <td>875.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>581012 rows × 54 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0      1     2      3      4       5   ...   48   49   50   51   52   53\n",
              "0       2596.0   51.0   3.0  258.0    0.0   510.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "1       2590.0   56.0   2.0  212.0   -6.0   390.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "2       2804.0  139.0   9.0  268.0   65.0  3180.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "3       2785.0  155.0  18.0  242.0  118.0  3090.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "4       2595.0   45.0   2.0  153.0   -1.0   391.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "...        ...    ...   ...    ...    ...     ...  ...  ...  ...  ...  ...  ...  ...\n",
              "581007  2396.0  153.0  20.0   85.0   17.0   108.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "581008  2391.0  152.0  19.0   67.0   12.0    95.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "581009  2386.0  159.0  17.0   60.0    7.0    90.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "581010  2384.0  170.0  15.0   60.0    5.0    90.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "581011  2383.0  165.0  13.0   60.0    4.0    67.0  ...  0.0  0.0  0.0  0.0  0.0  0.0\n",
              "\n",
              "[581012 rows x 54 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1AdiOaQZ0dhf",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 439
        },
        "outputId": "982095c5-cd82-4330-fdbb-4b1a74347415"
      },
      "source": [
        "data = covtype_df\n",
        "data = data.apply(\n",
        "    lambda x: (x - x.mean()) / x.std()\n",
        ")\n",
        "data['target'] = covtype['target']-1\n",
        "data"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>49</th>\n",
              "      <th>50</th>\n",
              "      <th>51</th>\n",
              "      <th>52</th>\n",
              "      <th>53</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>-1.297804</td>\n",
              "      <td>-0.935156</td>\n",
              "      <td>-1.482819</td>\n",
              "      <td>-0.053767</td>\n",
              "      <td>-0.796272</td>\n",
              "      <td>-1.180145</td>\n",
              "      <td>0.330743</td>\n",
              "      <td>0.439143</td>\n",
              "      <td>0.142960</td>\n",
              "      <td>3.246280</td>\n",
              "      <td>1.108079</td>\n",
              "      <td>-0.232859</td>\n",
              "      <td>-0.879363</td>\n",
              "      <td>-0.260673</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>-0.114549</td>\n",
              "      <td>-0.09149</td>\n",
              "      <td>-0.147649</td>\n",
              "      <td>-0.0525</td>\n",
              "      <td>-0.106986</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.017555</td>\n",
              "      <td>-0.044475</td>\n",
              "      <td>-0.243947</td>\n",
              "      <td>-0.147734</td>\n",
              "      <td>-0.233216</td>\n",
              "      <td>-0.175866</td>\n",
              "      <td>-0.032125</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>-0.070148</td>\n",
              "      <td>-0.076971</td>\n",
              "      <td>-0.057264</td>\n",
              "      <td>-0.08348</td>\n",
              "      <td>-0.127256</td>\n",
              "      <td>-0.038005</td>\n",
              "      <td>-0.24686</td>\n",
              "      <td>-0.332219</td>\n",
              "      <td>-0.194973</td>\n",
              "      <td>-0.028574</td>\n",
              "      <td>-0.066903</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.040384</td>\n",
              "      <td>2.010334</td>\n",
              "      <td>-0.234031</td>\n",
              "      <td>-0.214979</td>\n",
              "      <td>-0.315238</td>\n",
              "      <td>-0.290284</td>\n",
              "      <td>-0.05273</td>\n",
              "      <td>-0.057143</td>\n",
              "      <td>-0.014313</td>\n",
              "      <td>-0.022653</td>\n",
              "      <td>-0.165956</td>\n",
              "      <td>-0.156014</td>\n",
              "      <td>-0.123653</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.319234</td>\n",
              "      <td>-0.890479</td>\n",
              "      <td>-1.616361</td>\n",
              "      <td>-0.270188</td>\n",
              "      <td>-0.899196</td>\n",
              "      <td>-1.257105</td>\n",
              "      <td>0.293388</td>\n",
              "      <td>0.590898</td>\n",
              "      <td>0.221341</td>\n",
              "      <td>3.205501</td>\n",
              "      <td>1.108079</td>\n",
              "      <td>-0.232859</td>\n",
              "      <td>-0.879363</td>\n",
              "      <td>-0.260673</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>-0.114549</td>\n",
              "      <td>-0.09149</td>\n",
              "      <td>-0.147649</td>\n",
              "      <td>-0.0525</td>\n",
              "      <td>-0.106986</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.017555</td>\n",
              "      <td>-0.044475</td>\n",
              "      <td>-0.243947</td>\n",
              "      <td>-0.147734</td>\n",
              "      <td>-0.233216</td>\n",
              "      <td>-0.175866</td>\n",
              "      <td>-0.032125</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>-0.070148</td>\n",
              "      <td>-0.076971</td>\n",
              "      <td>-0.057264</td>\n",
              "      <td>-0.08348</td>\n",
              "      <td>-0.127256</td>\n",
              "      <td>-0.038005</td>\n",
              "      <td>-0.24686</td>\n",
              "      <td>-0.332219</td>\n",
              "      <td>-0.194973</td>\n",
              "      <td>-0.028574</td>\n",
              "      <td>-0.066903</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.040384</td>\n",
              "      <td>2.010334</td>\n",
              "      <td>-0.234031</td>\n",
              "      <td>-0.214979</td>\n",
              "      <td>-0.315238</td>\n",
              "      <td>-0.290284</td>\n",
              "      <td>-0.05273</td>\n",
              "      <td>-0.057143</td>\n",
              "      <td>-0.014313</td>\n",
              "      <td>-0.022653</td>\n",
              "      <td>-0.165956</td>\n",
              "      <td>-0.156014</td>\n",
              "      <td>-0.123653</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>-0.554906</td>\n",
              "      <td>-0.148836</td>\n",
              "      <td>-0.681562</td>\n",
              "      <td>-0.006719</td>\n",
              "      <td>0.318742</td>\n",
              "      <td>0.532212</td>\n",
              "      <td>0.816363</td>\n",
              "      <td>0.742653</td>\n",
              "      <td>-0.196691</td>\n",
              "      <td>3.126963</td>\n",
              "      <td>1.108079</td>\n",
              "      <td>-0.232859</td>\n",
              "      <td>-0.879363</td>\n",
              "      <td>-0.260673</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>-0.114549</td>\n",
              "      <td>-0.09149</td>\n",
              "      <td>-0.147649</td>\n",
              "      <td>-0.0525</td>\n",
              "      <td>-0.106986</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.017555</td>\n",
              "      <td>-0.044475</td>\n",
              "      <td>-0.243947</td>\n",
              "      <td>-0.147734</td>\n",
              "      <td>4.287864</td>\n",
              "      <td>-0.175866</td>\n",
              "      <td>-0.032125</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>-0.070148</td>\n",
              "      <td>-0.076971</td>\n",
              "      <td>-0.057264</td>\n",
              "      <td>-0.08348</td>\n",
              "      <td>-0.127256</td>\n",
              "      <td>-0.038005</td>\n",
              "      <td>-0.24686</td>\n",
              "      <td>-0.332219</td>\n",
              "      <td>-0.194973</td>\n",
              "      <td>-0.028574</td>\n",
              "      <td>-0.066903</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.040384</td>\n",
              "      <td>-0.497429</td>\n",
              "      <td>-0.234031</td>\n",
              "      <td>-0.214979</td>\n",
              "      <td>-0.315238</td>\n",
              "      <td>-0.290284</td>\n",
              "      <td>-0.05273</td>\n",
              "      <td>-0.057143</td>\n",
              "      <td>-0.014313</td>\n",
              "      <td>-0.022653</td>\n",
              "      <td>-0.165956</td>\n",
              "      <td>-0.156014</td>\n",
              "      <td>-0.123653</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>-0.622767</td>\n",
              "      <td>-0.005869</td>\n",
              "      <td>0.520322</td>\n",
              "      <td>-0.129044</td>\n",
              "      <td>1.227907</td>\n",
              "      <td>0.474492</td>\n",
              "      <td>0.965785</td>\n",
              "      <td>0.742653</td>\n",
              "      <td>-0.536343</td>\n",
              "      <td>3.194928</td>\n",
              "      <td>1.108079</td>\n",
              "      <td>-0.232859</td>\n",
              "      <td>-0.879363</td>\n",
              "      <td>-0.260673</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>-0.114549</td>\n",
              "      <td>-0.09149</td>\n",
              "      <td>-0.147649</td>\n",
              "      <td>-0.0525</td>\n",
              "      <td>-0.106986</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.017555</td>\n",
              "      <td>-0.044475</td>\n",
              "      <td>-0.243947</td>\n",
              "      <td>-0.147734</td>\n",
              "      <td>-0.233216</td>\n",
              "      <td>-0.175866</td>\n",
              "      <td>-0.032125</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>-0.070148</td>\n",
              "      <td>-0.076971</td>\n",
              "      <td>-0.057264</td>\n",
              "      <td>-0.08348</td>\n",
              "      <td>-0.127256</td>\n",
              "      <td>-0.038005</td>\n",
              "      <td>-0.24686</td>\n",
              "      <td>-0.332219</td>\n",
              "      <td>-0.194973</td>\n",
              "      <td>-0.028574</td>\n",
              "      <td>-0.066903</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.040384</td>\n",
              "      <td>-0.497429</td>\n",
              "      <td>4.272927</td>\n",
              "      <td>-0.214979</td>\n",
              "      <td>-0.315238</td>\n",
              "      <td>-0.290284</td>\n",
              "      <td>-0.05273</td>\n",
              "      <td>-0.057143</td>\n",
              "      <td>-0.014313</td>\n",
              "      <td>-0.022653</td>\n",
              "      <td>-0.165956</td>\n",
              "      <td>-0.156014</td>\n",
              "      <td>-0.123653</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-1.301376</td>\n",
              "      <td>-0.988769</td>\n",
              "      <td>-1.616361</td>\n",
              "      <td>-0.547770</td>\n",
              "      <td>-0.813426</td>\n",
              "      <td>-1.256463</td>\n",
              "      <td>0.293388</td>\n",
              "      <td>0.540313</td>\n",
              "      <td>0.195214</td>\n",
              "      <td>3.165476</td>\n",
              "      <td>1.108079</td>\n",
              "      <td>-0.232859</td>\n",
              "      <td>-0.879363</td>\n",
              "      <td>-0.260673</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>-0.114549</td>\n",
              "      <td>-0.09149</td>\n",
              "      <td>-0.147649</td>\n",
              "      <td>-0.0525</td>\n",
              "      <td>-0.106986</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.017555</td>\n",
              "      <td>-0.044475</td>\n",
              "      <td>-0.243947</td>\n",
              "      <td>-0.147734</td>\n",
              "      <td>-0.233216</td>\n",
              "      <td>-0.175866</td>\n",
              "      <td>-0.032125</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>-0.070148</td>\n",
              "      <td>-0.076971</td>\n",
              "      <td>-0.057264</td>\n",
              "      <td>-0.08348</td>\n",
              "      <td>-0.127256</td>\n",
              "      <td>-0.038005</td>\n",
              "      <td>-0.24686</td>\n",
              "      <td>-0.332219</td>\n",
              "      <td>-0.194973</td>\n",
              "      <td>-0.028574</td>\n",
              "      <td>-0.066903</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.040384</td>\n",
              "      <td>2.010334</td>\n",
              "      <td>-0.234031</td>\n",
              "      <td>-0.214979</td>\n",
              "      <td>-0.315238</td>\n",
              "      <td>-0.290284</td>\n",
              "      <td>-0.05273</td>\n",
              "      <td>-0.057143</td>\n",
              "      <td>-0.014313</td>\n",
              "      <td>-0.022653</td>\n",
              "      <td>-0.165956</td>\n",
              "      <td>-0.156014</td>\n",
              "      <td>-0.123653</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581007</th>\n",
              "      <td>-2.012129</td>\n",
              "      <td>-0.023740</td>\n",
              "      <td>0.787407</td>\n",
              "      <td>-0.867696</td>\n",
              "      <td>-0.504653</td>\n",
              "      <td>-1.437960</td>\n",
              "      <td>1.040496</td>\n",
              "      <td>0.692068</td>\n",
              "      <td>-0.640851</td>\n",
              "      <td>-0.863386</td>\n",
              "      <td>-0.902461</td>\n",
              "      <td>-0.232859</td>\n",
              "      <td>1.137185</td>\n",
              "      <td>-0.260673</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>8.729878</td>\n",
              "      <td>-0.09149</td>\n",
              "      <td>-0.147649</td>\n",
              "      <td>-0.0525</td>\n",
              "      <td>-0.106986</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.017555</td>\n",
              "      <td>-0.044475</td>\n",
              "      <td>-0.243947</td>\n",
              "      <td>-0.147734</td>\n",
              "      <td>-0.233216</td>\n",
              "      <td>-0.175866</td>\n",
              "      <td>-0.032125</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>-0.070148</td>\n",
              "      <td>-0.076971</td>\n",
              "      <td>-0.057264</td>\n",
              "      <td>-0.08348</td>\n",
              "      <td>-0.127256</td>\n",
              "      <td>-0.038005</td>\n",
              "      <td>-0.24686</td>\n",
              "      <td>-0.332219</td>\n",
              "      <td>-0.194973</td>\n",
              "      <td>-0.028574</td>\n",
              "      <td>-0.066903</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.040384</td>\n",
              "      <td>-0.497429</td>\n",
              "      <td>-0.234031</td>\n",
              "      <td>-0.214979</td>\n",
              "      <td>-0.315238</td>\n",
              "      <td>-0.290284</td>\n",
              "      <td>-0.05273</td>\n",
              "      <td>-0.057143</td>\n",
              "      <td>-0.014313</td>\n",
              "      <td>-0.022653</td>\n",
              "      <td>-0.165956</td>\n",
              "      <td>-0.156014</td>\n",
              "      <td>-0.123653</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581008</th>\n",
              "      <td>-2.029987</td>\n",
              "      <td>-0.032675</td>\n",
              "      <td>0.653865</td>\n",
              "      <td>-0.952382</td>\n",
              "      <td>-0.590423</td>\n",
              "      <td>-1.446298</td>\n",
              "      <td>1.040496</td>\n",
              "      <td>0.692068</td>\n",
              "      <td>-0.614724</td>\n",
              "      <td>-0.857344</td>\n",
              "      <td>-0.902461</td>\n",
              "      <td>-0.232859</td>\n",
              "      <td>1.137185</td>\n",
              "      <td>-0.260673</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>8.729878</td>\n",
              "      <td>-0.09149</td>\n",
              "      <td>-0.147649</td>\n",
              "      <td>-0.0525</td>\n",
              "      <td>-0.106986</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.017555</td>\n",
              "      <td>-0.044475</td>\n",
              "      <td>-0.243947</td>\n",
              "      <td>-0.147734</td>\n",
              "      <td>-0.233216</td>\n",
              "      <td>-0.175866</td>\n",
              "      <td>-0.032125</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>-0.070148</td>\n",
              "      <td>-0.076971</td>\n",
              "      <td>-0.057264</td>\n",
              "      <td>-0.08348</td>\n",
              "      <td>-0.127256</td>\n",
              "      <td>-0.038005</td>\n",
              "      <td>-0.24686</td>\n",
              "      <td>-0.332219</td>\n",
              "      <td>-0.194973</td>\n",
              "      <td>-0.028574</td>\n",
              "      <td>-0.066903</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.040384</td>\n",
              "      <td>-0.497429</td>\n",
              "      <td>-0.234031</td>\n",
              "      <td>-0.214979</td>\n",
              "      <td>-0.315238</td>\n",
              "      <td>-0.290284</td>\n",
              "      <td>-0.05273</td>\n",
              "      <td>-0.057143</td>\n",
              "      <td>-0.014313</td>\n",
              "      <td>-0.022653</td>\n",
              "      <td>-0.165956</td>\n",
              "      <td>-0.156014</td>\n",
              "      <td>-0.123653</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581009</th>\n",
              "      <td>-2.047845</td>\n",
              "      <td>0.029873</td>\n",
              "      <td>0.386779</td>\n",
              "      <td>-0.985316</td>\n",
              "      <td>-0.676193</td>\n",
              "      <td>-1.449504</td>\n",
              "      <td>0.891074</td>\n",
              "      <td>0.894408</td>\n",
              "      <td>-0.327326</td>\n",
              "      <td>-0.850548</td>\n",
              "      <td>-0.902461</td>\n",
              "      <td>-0.232859</td>\n",
              "      <td>1.137185</td>\n",
              "      <td>-0.260673</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>8.729878</td>\n",
              "      <td>-0.09149</td>\n",
              "      <td>-0.147649</td>\n",
              "      <td>-0.0525</td>\n",
              "      <td>-0.106986</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.017555</td>\n",
              "      <td>-0.044475</td>\n",
              "      <td>-0.243947</td>\n",
              "      <td>-0.147734</td>\n",
              "      <td>-0.233216</td>\n",
              "      <td>-0.175866</td>\n",
              "      <td>-0.032125</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>-0.070148</td>\n",
              "      <td>-0.076971</td>\n",
              "      <td>-0.057264</td>\n",
              "      <td>-0.08348</td>\n",
              "      <td>-0.127256</td>\n",
              "      <td>-0.038005</td>\n",
              "      <td>-0.24686</td>\n",
              "      <td>-0.332219</td>\n",
              "      <td>-0.194973</td>\n",
              "      <td>-0.028574</td>\n",
              "      <td>-0.066903</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.040384</td>\n",
              "      <td>-0.497429</td>\n",
              "      <td>-0.234031</td>\n",
              "      <td>-0.214979</td>\n",
              "      <td>-0.315238</td>\n",
              "      <td>-0.290284</td>\n",
              "      <td>-0.05273</td>\n",
              "      <td>-0.057143</td>\n",
              "      <td>-0.014313</td>\n",
              "      <td>-0.022653</td>\n",
              "      <td>-0.165956</td>\n",
              "      <td>-0.156014</td>\n",
              "      <td>-0.123653</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581010</th>\n",
              "      <td>-2.054988</td>\n",
              "      <td>0.128163</td>\n",
              "      <td>0.119694</td>\n",
              "      <td>-0.985316</td>\n",
              "      <td>-0.710502</td>\n",
              "      <td>-1.449504</td>\n",
              "      <td>0.666942</td>\n",
              "      <td>1.096748</td>\n",
              "      <td>0.012325</td>\n",
              "      <td>-0.842996</td>\n",
              "      <td>-0.902461</td>\n",
              "      <td>-0.232859</td>\n",
              "      <td>1.137185</td>\n",
              "      <td>-0.260673</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>8.729878</td>\n",
              "      <td>-0.09149</td>\n",
              "      <td>-0.147649</td>\n",
              "      <td>-0.0525</td>\n",
              "      <td>-0.106986</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.017555</td>\n",
              "      <td>-0.044475</td>\n",
              "      <td>-0.243947</td>\n",
              "      <td>-0.147734</td>\n",
              "      <td>-0.233216</td>\n",
              "      <td>-0.175866</td>\n",
              "      <td>-0.032125</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>-0.070148</td>\n",
              "      <td>-0.076971</td>\n",
              "      <td>-0.057264</td>\n",
              "      <td>-0.08348</td>\n",
              "      <td>-0.127256</td>\n",
              "      <td>-0.038005</td>\n",
              "      <td>-0.24686</td>\n",
              "      <td>-0.332219</td>\n",
              "      <td>-0.194973</td>\n",
              "      <td>-0.028574</td>\n",
              "      <td>-0.066903</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.040384</td>\n",
              "      <td>-0.497429</td>\n",
              "      <td>-0.234031</td>\n",
              "      <td>-0.214979</td>\n",
              "      <td>-0.315238</td>\n",
              "      <td>-0.290284</td>\n",
              "      <td>-0.05273</td>\n",
              "      <td>-0.057143</td>\n",
              "      <td>-0.014313</td>\n",
              "      <td>-0.022653</td>\n",
              "      <td>-0.165956</td>\n",
              "      <td>-0.156014</td>\n",
              "      <td>-0.123653</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>581011</th>\n",
              "      <td>-2.058560</td>\n",
              "      <td>0.083486</td>\n",
              "      <td>-0.147392</td>\n",
              "      <td>-0.985316</td>\n",
              "      <td>-0.727656</td>\n",
              "      <td>-1.464255</td>\n",
              "      <td>0.704297</td>\n",
              "      <td>1.046163</td>\n",
              "      <td>-0.039929</td>\n",
              "      <td>-0.834689</td>\n",
              "      <td>-0.902461</td>\n",
              "      <td>-0.232859</td>\n",
              "      <td>1.137185</td>\n",
              "      <td>-0.260673</td>\n",
              "      <td>-0.072416</td>\n",
              "      <td>8.729878</td>\n",
              "      <td>-0.09149</td>\n",
              "      <td>-0.147649</td>\n",
              "      <td>-0.0525</td>\n",
              "      <td>-0.106986</td>\n",
              "      <td>-0.013444</td>\n",
              "      <td>-0.017555</td>\n",
              "      <td>-0.044475</td>\n",
              "      <td>-0.243947</td>\n",
              "      <td>-0.147734</td>\n",
              "      <td>-0.233216</td>\n",
              "      <td>-0.175866</td>\n",
              "      <td>-0.032125</td>\n",
              "      <td>-0.002272</td>\n",
              "      <td>-0.070148</td>\n",
              "      <td>-0.076971</td>\n",
              "      <td>-0.057264</td>\n",
              "      <td>-0.08348</td>\n",
              "      <td>-0.127256</td>\n",
              "      <td>-0.038005</td>\n",
              "      <td>-0.24686</td>\n",
              "      <td>-0.332219</td>\n",
              "      <td>-0.194973</td>\n",
              "      <td>-0.028574</td>\n",
              "      <td>-0.066903</td>\n",
              "      <td>-0.043274</td>\n",
              "      <td>-0.040384</td>\n",
              "      <td>-0.497429</td>\n",
              "      <td>-0.234031</td>\n",
              "      <td>-0.214979</td>\n",
              "      <td>-0.315238</td>\n",
              "      <td>-0.290284</td>\n",
              "      <td>-0.05273</td>\n",
              "      <td>-0.057143</td>\n",
              "      <td>-0.014313</td>\n",
              "      <td>-0.022653</td>\n",
              "      <td>-0.165956</td>\n",
              "      <td>-0.156014</td>\n",
              "      <td>-0.123653</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>581012 rows × 55 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               0         1         2  ...        52        53  target\n",
              "0      -1.297804 -0.935156 -1.482819  ... -0.156014 -0.123653       4\n",
              "1      -1.319234 -0.890479 -1.616361  ... -0.156014 -0.123653       4\n",
              "2      -0.554906 -0.148836 -0.681562  ... -0.156014 -0.123653       1\n",
              "3      -0.622767 -0.005869  0.520322  ... -0.156014 -0.123653       1\n",
              "4      -1.301376 -0.988769 -1.616361  ... -0.156014 -0.123653       4\n",
              "...          ...       ...       ...  ...       ...       ...     ...\n",
              "581007 -2.012129 -0.023740  0.787407  ... -0.156014 -0.123653       2\n",
              "581008 -2.029987 -0.032675  0.653865  ... -0.156014 -0.123653       2\n",
              "581009 -2.047845  0.029873  0.386779  ... -0.156014 -0.123653       2\n",
              "581010 -2.054988  0.128163  0.119694  ... -0.156014 -0.123653       2\n",
              "581011 -2.058560  0.083486 -0.147392  ... -0.156014 -0.123653       2\n",
              "\n",
              "[581012 rows x 55 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxZPDbuL0fWn",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "4fc9bfaf-1467-4f6d-ebb2-f74eaa5fa019"
      },
      "source": [
        "X,y = data.values[:,:-1],data.values[:,-1:]\n",
        "print(X.shape,y.shape)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(581012, 54) (581012, 1)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NuWeAC5t0gn3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wTtgNwiI0jL3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class MyDataset(Dataset):\n",
        "    def __init__(self):\n",
        "        self.x_data = torch.FloatTensor(X)\n",
        "        self.y_data = torch.LongTensor(y)\n",
        "    def __len__(self):\n",
        "        return len(self.x_data)\n",
        "\n",
        "    def __getitem__(self,idx):\n",
        "        x = self.x_data[idx]\n",
        "        y = self.y_data[idx]\n",
        "        return x,y"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NFDkXNO00mHN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = MyDataset()\n",
        "dataloader = DataLoader(dataset, batch_size=len(dataset), shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bCzWBCK80oLu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SoftmaxClassifierModel(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.linear = nn.Linear(54, 7)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.linear(x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8mxKzyu0pck",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = SoftmaxClassifierModel()\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
        "criterion = nn.CrossEntropyLoss()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YlftA0eX0tbe",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "833950db-ddf2-403b-aa86-49d495512a0f"
      },
      "source": [
        "nb_epochs = 100\n",
        "for epoch in range(nb_epochs + 1):\n",
        "    for x_train,y_train in dataloader:\n",
        "        y_train = y_train.squeeze(1)\n",
        "        # H(x) 계산\n",
        "        hypothesis = model(x_train)\n",
        "        # cost 계산\n",
        "        cost=criterion(hypothesis, y_train)\n",
        "        # cost로 H(x) 개선\n",
        "        optimizer.zero_grad()\n",
        "        cost.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        correct_prediction = torch.argmax(hypothesis,dim=1) == y_train # 실제값과 일치하는 경우만 True로 간주\n",
        "        accuracy = correct_prediction.sum().item() / len(correct_prediction) # 정확도를 계산\n",
        "        print(f'Epoch : {epoch}/{nb_epochs}, Cost: {cost.item()}, Accuracy : {accuracy * 100}%')\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch : 0/100, Cost: 2.105910062789917, Accuracy : 13.87871506956827%\n",
            "Epoch : 1/100, Cost: 2.045290470123291, Accuracy : 18.061072748927735%\n",
            "Epoch : 2/100, Cost: 1.9877128601074219, Accuracy : 21.919340736508026%\n",
            "Epoch : 3/100, Cost: 1.9330474138259888, Accuracy : 25.009294128176357%\n",
            "Epoch : 4/100, Cost: 1.8810876607894897, Accuracy : 27.67946273054601%\n",
            "Epoch : 5/100, Cost: 1.8318874835968018, Accuracy : 29.966334602383426%\n",
            "Epoch : 6/100, Cost: 1.785164475440979, Accuracy : 32.12085120445017%\n",
            "Epoch : 7/100, Cost: 1.7408995628356934, Accuracy : 34.5853441925468%\n",
            "Epoch : 8/100, Cost: 1.6990392208099365, Accuracy : 37.56118634382767%\n",
            "Epoch : 9/100, Cost: 1.659314751625061, Accuracy : 40.41362312654472%\n",
            "Epoch : 10/100, Cost: 1.6217429637908936, Accuracy : 43.98532216202075%\n",
            "Epoch : 11/100, Cost: 1.5861486196517944, Accuracy : 47.78317831645474%\n",
            "Epoch : 12/100, Cost: 1.552445411682129, Accuracy : 51.271058084858836%\n",
            "Epoch : 13/100, Cost: 1.5205663442611694, Accuracy : 54.31540140306913%\n",
            "Epoch : 14/100, Cost: 1.4903053045272827, Accuracy : 56.82326698932207%\n",
            "Epoch : 15/100, Cost: 1.4617235660552979, Accuracy : 58.65868519066731%\n",
            "Epoch : 16/100, Cost: 1.4345453977584839, Accuracy : 60.03421616076776%\n",
            "Epoch : 17/100, Cost: 1.4087821245193481, Accuracy : 61.196154296296804%\n",
            "Epoch : 18/100, Cost: 1.3844013214111328, Accuracy : 62.0038828802159%\n",
            "Epoch : 19/100, Cost: 1.3611913919448853, Accuracy : 62.570136245034526%\n",
            "Epoch : 20/100, Cost: 1.339241862297058, Accuracy : 63.05549627202191%\n",
            "Epoch : 21/100, Cost: 1.318320393562317, Accuracy : 63.53638134840589%\n",
            "Epoch : 22/100, Cost: 1.2984789609909058, Accuracy : 63.90711379455157%\n",
            "Epoch : 23/100, Cost: 1.2795636653900146, Accuracy : 64.21450847831026%\n",
            "Epoch : 24/100, Cost: 1.2616606950759888, Accuracy : 64.47181813800748%\n",
            "Epoch : 25/100, Cost: 1.2445369958877563, Accuracy : 64.65856126895831%\n",
            "Epoch : 26/100, Cost: 1.2282466888427734, Accuracy : 64.83566604476327%\n",
            "Epoch : 27/100, Cost: 1.2126715183258057, Accuracy : 65.00399303284614%\n",
            "Epoch : 28/100, Cost: 1.1979084014892578, Accuracy : 65.16698450290184%\n",
            "Epoch : 29/100, Cost: 1.1838220357894897, Accuracy : 65.3477036618865%\n",
            "Epoch : 30/100, Cost: 1.1703178882598877, Accuracy : 65.51482585557613%\n",
            "Epoch : 31/100, Cost: 1.1574207544326782, Accuracy : 65.65664736700792%\n",
            "Epoch : 32/100, Cost: 1.1450704336166382, Accuracy : 65.78142964344971%\n",
            "Epoch : 33/100, Cost: 1.133335828781128, Accuracy : 65.9254886301832%\n",
            "Epoch : 34/100, Cost: 1.1220619678497314, Accuracy : 66.07023607085569%\n",
            "Epoch : 35/100, Cost: 1.1112698316574097, Accuracy : 66.19398566638898%\n",
            "Epoch : 36/100, Cost: 1.1008943319320679, Accuracy : 66.29484416845091%\n",
            "Epoch : 37/100, Cost: 1.0909202098846436, Accuracy : 66.3592146117464%\n",
            "Epoch : 38/100, Cost: 1.081398606300354, Accuracy : 66.41497938080452%\n",
            "Epoch : 39/100, Cost: 1.0722461938858032, Accuracy : 66.48933240621537%\n",
            "Epoch : 40/100, Cost: 1.0634678602218628, Accuracy : 66.55611243829732%\n",
            "Epoch : 41/100, Cost: 1.0550050735473633, Accuracy : 66.61686849841311%\n",
            "Epoch : 42/100, Cost: 1.0468710660934448, Accuracy : 66.66471604717287%\n",
            "Epoch : 43/100, Cost: 1.0390487909317017, Accuracy : 66.71428473078008%\n",
            "Epoch : 44/100, Cost: 1.0315381288528442, Accuracy : 66.76368130090256%\n",
            "Epoch : 45/100, Cost: 1.0242689847946167, Accuracy : 66.80154626754697%\n",
            "Epoch : 46/100, Cost: 1.017252802848816, Accuracy : 66.84388618479481%\n",
            "Epoch : 47/100, Cost: 1.0105139017105103, Accuracy : 66.88949625825285%\n",
            "Epoch : 48/100, Cost: 1.003989577293396, Accuracy : 66.93820437443632%\n",
            "Epoch : 49/100, Cost: 0.997678816318512, Accuracy : 66.9781347028977%\n",
            "Epoch : 50/100, Cost: 0.9916021823883057, Accuracy : 67.02357266287099%\n",
            "Epoch : 51/100, Cost: 0.9857457876205444, Accuracy : 67.0770999566274%\n",
            "Epoch : 52/100, Cost: 0.9800957441329956, Accuracy : 67.1320041582618%\n",
            "Epoch : 53/100, Cost: 0.9745945334434509, Accuracy : 67.19516980716406%\n",
            "Epoch : 54/100, Cost: 0.969267725944519, Accuracy : 67.24732019304248%\n",
            "Epoch : 55/100, Cost: 0.9641140103340149, Accuracy : 67.29568408225649%\n",
            "Epoch : 56/100, Cost: 0.9591356515884399, Accuracy : 67.35454689404006%\n",
            "Epoch : 57/100, Cost: 0.9543152451515198, Accuracy : 67.41943367778978%\n",
            "Epoch : 58/100, Cost: 0.9496451020240784, Accuracy : 67.47141195018347%\n",
            "Epoch : 59/100, Cost: 0.9450968503952026, Accuracy : 67.51306341349232%\n",
            "Epoch : 60/100, Cost: 0.940700888633728, Accuracy : 67.56297632406904%\n",
            "Epoch : 61/100, Cost: 0.9364368915557861, Accuracy : 67.60084129071345%\n",
            "Epoch : 62/100, Cost: 0.9322865605354309, Accuracy : 67.65195899568339%\n",
            "Epoch : 63/100, Cost: 0.9282366633415222, Accuracy : 67.70428149504657%\n",
            "Epoch : 64/100, Cost: 0.9243327975273132, Accuracy : 67.7629721933454%\n",
            "Epoch : 65/100, Cost: 0.9205432534217834, Accuracy : 67.81598314664757%\n",
            "Epoch : 66/100, Cost: 0.9168463945388794, Accuracy : 67.86297012798359%\n",
            "Epoch : 67/100, Cost: 0.913245677947998, Accuracy : 67.91357149249929%\n",
            "Epoch : 68/100, Cost: 0.9097567200660706, Accuracy : 67.95608352323188%\n",
            "Epoch : 69/100, Cost: 0.9063672423362732, Accuracy : 68.00461952593062%\n",
            "Epoch : 70/100, Cost: 0.9030516147613525, Accuracy : 68.04472196787674%\n",
            "Epoch : 71/100, Cost: 0.8998259902000427, Accuracy : 68.08757822557881%\n",
            "Epoch : 72/100, Cost: 0.8966847658157349, Accuracy : 68.12509896525373%\n",
            "Epoch : 73/100, Cost: 0.8936299085617065, Accuracy : 68.1715696061355%\n",
            "Epoch : 74/100, Cost: 0.8906598091125488, Accuracy : 68.20668075702395%\n",
            "Epoch : 75/100, Cost: 0.8877568244934082, Accuracy : 68.25297928442097%\n",
            "Epoch : 76/100, Cost: 0.8849384188652039, Accuracy : 68.28929522970265%\n",
            "Epoch : 77/100, Cost: 0.8821619749069214, Accuracy : 68.3254390614996%\n",
            "Epoch : 78/100, Cost: 0.8794809579849243, Accuracy : 68.36743475177794%\n",
            "Epoch : 79/100, Cost: 0.8768406510353088, Accuracy : 68.40891410160204%\n",
            "Epoch : 80/100, Cost: 0.8742799162864685, Accuracy : 68.44557427385321%\n",
            "Epoch : 81/100, Cost: 0.8717741370201111, Accuracy : 68.47810372247045%\n",
            "Epoch : 82/100, Cost: 0.8693245053291321, Accuracy : 68.51751771047758%\n",
            "Epoch : 83/100, Cost: 0.866950511932373, Accuracy : 68.55314520182027%\n",
            "Epoch : 84/100, Cost: 0.8646357655525208, Accuracy : 68.59341975725114%\n",
            "Epoch : 85/100, Cost: 0.8623546361923218, Accuracy : 68.61958100693273%\n",
            "Epoch : 86/100, Cost: 0.8601105809211731, Accuracy : 68.6464307105533%\n",
            "Epoch : 87/100, Cost: 0.8579385876655579, Accuracy : 68.68911485477064%\n",
            "Epoch : 88/100, Cost: 0.855820894241333, Accuracy : 68.72405389217434%\n",
            "Epoch : 89/100, Cost: 0.8537266850471497, Accuracy : 68.7526247306424%\n",
            "Epoch : 90/100, Cost: 0.8516949415206909, Accuracy : 68.78153979607994%\n",
            "Epoch : 91/100, Cost: 0.8496977090835571, Accuracy : 68.81699517393788%\n",
            "Epoch : 92/100, Cost: 0.8477452397346497, Accuracy : 68.85227843831109%\n",
            "Epoch : 93/100, Cost: 0.8458354473114014, Accuracy : 68.88429154647409%\n",
            "Epoch : 94/100, Cost: 0.843950629234314, Accuracy : 68.91320661191162%\n",
            "Epoch : 95/100, Cost: 0.8421385288238525, Accuracy : 68.94900621673908%\n",
            "Epoch : 96/100, Cost: 0.8403295874595642, Accuracy : 68.9861827294445%\n",
            "Epoch : 97/100, Cost: 0.8385748267173767, Accuracy : 69.01630258927527%\n",
            "Epoch : 98/100, Cost: 0.8368534445762634, Accuracy : 69.04349651986534%\n",
            "Epoch : 99/100, Cost: 0.835166871547699, Accuracy : 69.07361637969612%\n",
            "Epoch : 100/100, Cost: 0.8335176706314087, Accuracy : 69.09788438104549%\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "29sYQ789hlfa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}